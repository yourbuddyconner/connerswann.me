<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Conner Swann</title><link href="https://connerswann.me/" rel="alternate"></link><link href="https://connerswann.me/feeds/all.atom.xml" rel="self"></link><id>https://connerswann.me/</id><updated>2021-05-01T00:00:00-07:00</updated><subtitle>Reliability Engineer</subtitle><entry><title>Celo: The Infra Running the Espero Ceremony</title><link href="https://connerswann.me/2021/05/celo-espero-infrastructure.html" rel="alternate"></link><published>2021-05-01T00:00:00-07:00</published><updated>2021-05-01T00:00:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2021-05-01:/2021/05/celo-espero-infrastructure.html</id><summary type="html">&lt;h1&gt;Who is This Post For?&lt;/h1&gt;
&lt;p&gt;This article was designed for technical readers who are interested in doing any of the following: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Someone who is going to fork Espero and run their own MPC Ceremony.&lt;/li&gt;
&lt;li&gt;Someone who has participated in MPCs in the past and wants to know how Espero is …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1&gt;Who is This Post For?&lt;/h1&gt;
&lt;p&gt;This article was designed for technical readers who are interested in doing any of the following: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Someone who is going to fork Espero and run their own MPC Ceremony.&lt;/li&gt;
&lt;li&gt;Someone who has participated in MPCs in the past and wants to know how Espero is &lt;em&gt;different&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Someone who wants to use the outputs from Phase 1 of the Espero Ceremony and use it to do their own Phase 2 setup &lt;/li&gt;
&lt;li&gt;Someone who use the output of Phase 1 with a universal SNARK such as PLONK or Marlin.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This initial post is a general primer on Plumo and the Espero Ceremony, what they do, and how it all works — from both a high-level perspective and that of the Human operator of the ceremony.  Another post will follow describing the strategy one must have when considering the prospect of their own Phase 2 ceremony. &lt;/p&gt;
&lt;h1&gt;What is Celo?&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://celo.org/"&gt;Celo&lt;/a&gt; is a Cryptocurrency protocol that I contribute to as part of the cLabs team. cLabs's Mission is to ensure the conditions of "Financial Prosperity for all," which we are executing on by building the ecosystem's first mobile-first blockchain. For many people globally, banking and payments infrastructure is non-existent or hard to get access to -- Celo's goal is to fill that gap. &lt;/p&gt;
&lt;p&gt;In the words of &lt;a href="https://www.coindesk.com/libra-minus-facebook-why-celo-is-2020s-buzzy-token-project"&gt;Polychain Capital President, Joe Eagan&lt;/a&gt;, “The Celo mobile app, bringing stable payments and remittances to the unbanked, has a chance to bring broad swaths of people to the world of blockchain technology and crypto, but to also level up the economic well-being of those most in need across the globe."&lt;/p&gt;
&lt;p&gt;For more details on what Celo is doing and how it does it, I highly recommend checking out &lt;a href="https://medium.com/celoorg/why-build-on-the-celo-blockchain-9ceab3d11b70"&gt;this post&lt;/a&gt; on the cLabs Blog by the cLabs CTO Marek Olszewski.&lt;/p&gt;
&lt;h2&gt;What is Plumo?&lt;/h2&gt;
&lt;p&gt;Since I am hardly an expert on the cryptography involved here, I will mostly be quoting &lt;a href="https://medium.com/celoorg/the-plumo-ceremony-ac7649e9c8d8"&gt;this great post&lt;/a&gt; on the cLabs Engineering Blog about Plumo.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Plumo is a zero-knowledge SNARK based syncing protocol that takes the “lightest sync” mode to the next level. By reducing the time and data needed to sync the blockchain by multiple orders of magnitude, Plumo enables even the most resource constrained mobile devices to transact trustlessly on the Celo network.&lt;/p&gt;
&lt;p&gt;The SNARK at the heart of Plumo enables light clients to sync with the Celo network via ultra-light sync mode. A single SNARK generated by the Plumo protocol can verify over 100 epoch headers instantaneously, allowing light clients to verify this in a quick, light, and trustless way. This results in sync speeds that are orders of magnitude faster, with improvements in the amount of data you need to sync with the Celo network by a factor of around 1,000,000 relative to other networks.&lt;/p&gt;
&lt;p&gt;Plumo is one of the largest SNARKs to be deployed. Internally, it proves the correctness of the evolution of Celo epochs, just as a light client would have seen. This means that essentially it proves the light client protocol. It does this through verifying over 100 BLS signatures that use a combination of a Bowe-Hopwood variant of a Pedersen hash and Blake2s for its hash to curve, and performs the consistency checks of validator elections, asserting that ⅔ of the validator set agreed on the next set. Plumo uses the two-chain of BLS12–377 and BW6 to create BW6 proofs that ultra-light clients verify. This allows them to verify a large number of epochs by verifying a single SNARK proof.&lt;/p&gt;
&lt;p&gt;The SNARK requires one final step before it can go live on mainnet — the Plumo MPC Ceremony, which requires the help of Celo friends and community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;What is Espero?&lt;/h1&gt;
&lt;p&gt;Espero is the proper noun used to describe the Plumo Ceremony, again quoting from the blog post: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Multi-Party-Computation (MPC) is a cryptographic mechanism for different parties to jointly perform a computation. SNARK circuits require a “trusted setup” where parties work together to generate shared parameters that can be used to prove and verify SNARKs. If one person ran this setup, then they could potentially prove incorrect things by exploiting a backdoor in the circuit. However, with an MPC, this setup process is split amongst tens or hundreds of contributors, and if even one of the participants is honest (keeps their inputs private), then the system will be secure.&lt;/p&gt;
&lt;p&gt;In the case of the Plumo Ceremony, this collective computation will be a series of joint actions done by a group of participants from within the Celo community and beyond. They will be working to perform an MPC that secures the SNARK proving the Plumo ultralight client protocol.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Espero improves on top of the existing MPC implementations by introducing the option of optimistic parallel computation. This is possible since contributions are &lt;em&gt;commutative&lt;/em&gt;. As in, it doesn’t matter in which order participants contribute. Additionally, elements being contributed to can also be worked on independently of one another. Utilizing these properties, Espero divides the contributions into smaller chunks on which participants work in parallel. That works well as long as all the participants contribute to all of the chunks. &lt;/p&gt;
&lt;p&gt;As usual, there’s no free lunch. If even one of the participants decides to stop contributing or maliciously makes a bad contribution, the whole set of contributions that were produced in parallel must be thrown away. This is an organizational consideration that must be made, but there is no risk to security since everything is verifiable. This kind of trade off makes sense when the parameter size is large and you want to make quick progress, but are willing to do more work as the coordinator of the setup to gain confidence that participants will complete their contribution across all the chunks.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ceremony will consist of rounds of about 6–10 participants each running the Plumo setup software for a certain period of time. Each round will last approximately 36 hours. While much of the activity is passive and involves simply running the computation on a desktop machine, for the Ceremony participants should feel confident with running commands in the terminal and destroying USB keys. This will allow participants to feel comfortable with the commitment so that the Ceremony can run smoothly.&lt;/p&gt;
&lt;p&gt;The SNARK being secured by the Plumo Ceremony is one of the most complex yet powerful SNARKs to ever be secured, and the outcome can be used not only by cLabs &amp;amp; the Plumo construction, but also by any project that uses the BW6 curve with the Groth16 proving system or any other based on polynomial commitments. By participating in this Ceremony, participants can be part of something that is a public good, and will hopefully be used to power many more systems to come.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Architecture Diagram&lt;/h2&gt;
&lt;p&gt;Before getting in too deep, let’s look at the system from a high level and identify all the major parts. Below is an architecture diagram outlining the system, including the Microsoft Azure services that were used. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Espero Ceremony Architecture Diagram" src="../../images/2021/espero-ceremony-architecture.png"&gt;&lt;/p&gt;
&lt;p&gt;Important Pieces: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ceremony Coordinator:&lt;/strong&gt; The brains of the whole ceremony, authenticates users via Public Key signatures and coordinates the metadata required to operate the ceremony efficiently. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ceremony Participant:&lt;/strong&gt; The brawn of the ceremony, it communicates via signed HTTP packets with the coordinator. It retrieves chunks to be processed and uploads them to Azure Block Storage when complete.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ceremony Verifier:&lt;/strong&gt; Communicates with the coordinator, waiting for new contributions and verifying them on the fly. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ceremony Monitor:&lt;/strong&gt; Observes changes in the Coordinator API, emitting logs that can be used for Alerting on important changes in ceremony state. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Azure Front Door:&lt;/strong&gt; Used for DDoS mitigation, easy to set up. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Azure Storage Account:&lt;/strong&gt; Used to store and retrieve chunks, also easy to set up and use. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes: and Helm&lt;/strong&gt;: Used to deploy and manage the various processes that must run in order to facilitate the ceremony. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terraform:&lt;/strong&gt; Used to wire up and deploy all the various pieces of infrastructure, including Kubernetes Clusters, Load Balancers, and other cloud resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Okay, But How Does it &lt;em&gt;Work&lt;/em&gt;?&lt;/h2&gt;
&lt;p&gt;At the end of the day, the Espero Ceremony is a Typescript JSON API Server that responds to HTTP Requests from various other actors in the system, secured with the same public key cryptography that is the basis for the Celo (and Ethereum) blockchain. Users who desire to contribute to the ceremony generate a keypair and share the public key with the Ceremony operator, then signing their HTTP requests for authentication. Optionally (but recommended), contributors may also publish an attestation to a public channel like Twitter (&lt;a href="https://twitter.com/YourBuddyConner/status/1339663701498982400"&gt;here's mine!&lt;/a&gt;) -- this is to ensure that all of the individuals participating are associated with unique real-world identities (or psuedo-identities) and not just one or more entities colluding. &lt;/p&gt;
&lt;p&gt;The Espero ceremony is particularly notable because it is split up into rounds, allowing a small number of participants to contribute their computations in parallel -- a novel innovation in the MPC ecosystem. Other MPC Ceremonies like the OG &lt;a href="https://github.com/ebfull/powersoftau"&gt;Perpetual Powers of Tau&lt;/a&gt; only allow for one participant at a time. While this architecture is not necessarily a limitation based on pure math, parallelizing it does allow for larger groups to be involved at once.&lt;/p&gt;
&lt;p&gt;Without getting too deep into the crypto, the gist of an MPC is you download something called a "Structured Reference String," do some computation with it as input, and return the output for the next participant to operate against. &lt;strong&gt;It's like a giant game of &lt;em&gt;crypto telephone&lt;/em&gt;, where each step along the way is 100% verifiable.&lt;/strong&gt; The Structured Reference String consists of four vectors of varying size (see below diagram), each containing field elements from one of two Groups &lt;code&gt;G1&lt;/code&gt; and &lt;code&gt;G2&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;In order to facilitate multiple users in a round, the Structured Reference String is split up into many &lt;em&gt;chunks&lt;/em&gt; which consist of slices of the field elements that are to be operated upon. Contributors acquire a "lock" on a chunk, download the chunk's contents, process the computation, and upload the results back to Azure to be verified. &lt;/p&gt;
&lt;p&gt;The below diagram describes logically how the chunks are split: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Espero Ceremony Infographic" src="../../images/2021/espero-ceremony-infographic.png"&gt;&lt;/p&gt;
&lt;p&gt;While it allows for parallelism in terms of how many users can participate, there is a major constraint to this architecture in that each chunk can only be operated upon by one participant at a time. This requires a high level of coordination by both the party operating the ceremony as well as the individual participating in the ceremony. &lt;/p&gt;
&lt;p&gt;Fortunately, we at cLabs have provided a handy &lt;a href="https://github.com/celo-org/snark-setup-operator"&gt;program, written in Rust&lt;/a&gt; that can be used to contribute. Pre-compiled versions of the code are provided on Github, but it is entirely possible (and encouraged) to both read the source code and compile the binary yourself!&lt;/p&gt;
&lt;p&gt;Below, we have a sequence diagram that gives a high-level overview of what the contribution binary is doing when you participate in the Plumo Setup.&lt;/p&gt;
&lt;script&gt;window.addEventListener("message", function(e) {var i = e.data.split(":")[1];var h = e.data.split(":")[2];if (e.data.split(":")[0] == "swimlanes-io" &amp;&amp; i &amp;&amp; h) {document.getElementById("__sw-io-" + i).setAttribute("style","height:" + h + "px");}}, false);&lt;/script&gt;
&lt;div id="__sw-io-lVHg"&gt;&lt;iframe style="border:none; width:100%; height:100%" scrolling="no" src="https://cdn.swimlanes.io/dist/embeded.html#lVHLUoMwFN3zFecHhFofCxbOWGSYagccaPfEcC2Z0gSTsNCvN9CHdWqrLnPvedxzYoVtKERsWtIKkZJWi5fOCiVR0FtHkhMeBFtqtva8/VppXNw5tNKVkMw9Q1z6yHuCseB1J1cGZRLPEXDStFbyvfQO4Bv2XixEtEXhschSlOPRCNlTedZw7GOm+ApR77YzG5yDUFRB43a/eA7EXoOqk5b3H50mTHo1FG7Glq6tq6+skdPwfgCdcOtHJK05TnYAvfbxrBUnYzasszXc+MfZ+Q5Pf2ng26cv8tn/urj1sWgbxSpXiWmVNOR5Ulm3yaklZmFrYdBu87w6JWK8Rie3I9f9cHkfI5tnmMTJNE2naVJ+Ag==#lVHg"&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;Since the ceremony coordinator is easily modelled as a traditional web service, it is trivial to then deploy it in containers to Kubernetes. For the remainder of this post, we're going to discuss the various parts of the deployment (included in the Open Source &lt;a href="https://github.com/celo-org/snark-setup-coordinator"&gt;Espero Coordinator Repository&lt;/a&gt;) in the hopes that it is both interesting and relevant to others who want to operate and/or extend the system. &lt;/p&gt;
&lt;h2&gt;Aside: Why Microsoft Azure?&lt;/h2&gt;
&lt;p&gt;Like any Cloud Platform, I see using Azure as a cost/benefit analysis for a given project. The major reason we at cLabs run workloads on Azure is because the Azure Hardware Security Module (HSM) supports the &lt;code&gt;secp256k1&lt;/code&gt; curve that is the basis for the public key cryptography in Celo (and the Espero Ceremony!). We use this feature for transaction signing among other things, and Google Cloud Platform (my cloud of choice) doesn't currently support this curve in their solution (booo!). Fortunately, Azure provides a perfectly serviceable (if incredibly opinionated/non-intuitive) Kubernetes environment for us to work with. &lt;/p&gt;
&lt;p&gt;Since we were already using Azure, we immediately took a couple tools off the shelf, including Azure Block Storage for chunk storage, Azure Front Door for DDoS mitigation (that we haven't required thus far), and Azure Kubernetes Service for workload management. &lt;/p&gt;
&lt;h2&gt;The Lifecycle of a Ceremony&lt;/h2&gt;
&lt;p&gt;Because there is some human intervention required in the Espero Ceremony, an additional actor is introduced called the &lt;code&gt;Operator&lt;/code&gt;. This entity, like the other actors in the system, authenticates to the Coordinator via private key. The entire lifecycle of the ceremony is managed by the &lt;code&gt;Coordinator&lt;/code&gt; and administered by the &lt;code&gt;Operator&lt;/code&gt;, including adding/removing participants, verifying each round, and transitioning between rounds.&lt;/p&gt;
&lt;p&gt;The operator commands are located in the control binary — so now let’s walk through initialization and the first round: &lt;/p&gt;
&lt;h3&gt;Ceremony Initialization&lt;/h3&gt;
&lt;p&gt;At the very beginning of the ceremony, chunks must be created and uploaded to Azure Block Storage. These make up the initial challenges that each participant in Round 0 will download, compute against, and re-upload. &lt;/p&gt;
&lt;h3&gt;Contributor Enrollment&lt;/h3&gt;
&lt;p&gt;The first thing we need to do after initializing the ceremony is add participants. We can do this by using the &lt;code&gt;add-participant&lt;/code&gt; method, passing an address generated via the &lt;code&gt;generate&lt;/code&gt; binary as the &lt;code&gt;--participant-id&lt;/code&gt; argument. &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
./target/release/control --coordinator-url https://plumo-example.azurefd.net \
--keys-file ~/plumo-verifier.keys \
add-participant \
--participant-id ADDRESS
&lt;/code&gt;
&lt;/pre&gt;

&lt;h3&gt;Round Maintenance&lt;/h3&gt;
&lt;p&gt;Besides adding users, the control binary can be used to unstick the coordinator which is capable of getting into states where the ceremony cannot proceed -- especially if all participants do not complete their contribution as planned. Additionally, it's important to note that while the Operator can remove data that has already been contributed, it cannot forge data on behalf of a participant. This is ensured by the verifiable public key cryptography that underpins the Espero Ceremony and Plumo SNARK. &lt;/p&gt;
&lt;p&gt;Due to the way the Structured Reference String is broken up, as mentioned previously, each participant must complete their section of the computation on each chunk in order for the ceremony to proceed. There are a plethora of reasons a user could drop out of the ceremony, from slow/inconsistent internet connections to hardware failure. &lt;/p&gt;
&lt;p&gt;If &lt;code&gt;Participant A&lt;/code&gt; drops out in the middle of the ceremony there are several options: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Have &lt;code&gt;Participant A&lt;/code&gt; share their private key with someone who may complete the contribution on their behalf, forfeiting some or all of the security guarantees of that particular contribution. &lt;/li&gt;
&lt;li&gt;Remove the participant, invalidating all chunks that &lt;code&gt;Participant A&lt;/code&gt; contributed and also invalidating any descendent chunks created by other participants that used &lt;code&gt;Participant A&lt;/code&gt;'s chunks as the basis for their own contributions. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This puts a strong emphasis on maintaining communication with participants and ensuring that private keys are not destroyed before the round is complete and verified. Removing users halfway through really degrades the user experience, so being proactive when this occurs can go a long way. &lt;/p&gt;
&lt;h3&gt;Round Verification&lt;/h3&gt;
&lt;p&gt;Because every contribution is signed and verifiable, the coordinator produces a &lt;em&gt;transcript&lt;/em&gt; that consists of every operation that was made. This transcript can then be verified by anyone as a chain of custody of sorts that shows the evolution of the Structured Reference String. &lt;/p&gt;
&lt;p&gt;Below is an example of a command that verifies a round: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
./target/release/control --coordinator-url https://plumo-example.azurefd.net \
--keys-file ~/plumo-verifier.keys new-round \
-e EXPECTED_PARTICIPANT_1 -e EXPECTED_PARTICIPANT_2 \
--verify-transcript \
--publish
&lt;/code&gt;
&lt;/pre&gt;

&lt;h3&gt;Alerting on Ceremony State&lt;/h3&gt;
&lt;p&gt;The Ceremony is structured such that participants are capable of shipping errors to the Coordinator, thereby notifying the Ceremony organizers of potential issues. The most common problem that pops up is simple networking issues, like localized DNS inconsistencies or global network partitions. These error logs are shipped to the Coordinator and show up in its logs to be consumed by alerting tools like the Azure Log Analytics Workspace. &lt;/p&gt;</content><category term="Celo Protocol"></category><category term="Celo"></category><category term="Plumo"></category><category term="SNARKs"></category><category term="Trusted Setup"></category></entry><entry><title>March to Mainnet: Sending a Transaction With Your Ledger Hardware Wallet</title><link href="https://connerswann.me/2021/02/mina-ledger-app.html" rel="alternate"></link><published>2021-02-22T00:00:00-08:00</published><updated>2021-02-22T00:00:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2021-02-22:/2021/02/mina-ledger-app.html</id><summary type="html">&lt;h1&gt;Need to Test your Ledger?&lt;/h1&gt;
&lt;p&gt;Mina Mainnet is upon us, if you are one of those who opted to self-custody their Mina tokens, chances are you generated your addresses with a Ledger Nano S. Now, your next step is to send some transactions with it, but you might be surprised …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Need to Test your Ledger?&lt;/h1&gt;
&lt;p&gt;Mina Mainnet is upon us, if you are one of those who opted to self-custody their Mina tokens, chances are you generated your addresses with a Ledger Nano S. Now, your next step is to send some transactions with it, but you might be surprised to learn that it's not quite as easy as running a Mina daemon locally! Don't worry, it's still pretty easy, but you need a helping hand from the Mina Archive node and the &lt;code&gt;mina-rosetta&lt;/code&gt; API. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Mainnet is Coming Meme" src="../../images/2021/mainnet-is-coming.jpg"&gt;&lt;/p&gt;
&lt;p&gt;In this installment of the Mina &lt;em&gt;March to Mainnet&lt;/em&gt; series we will be covering how to send transactions with a Ledger hardware wallet. We'll also introduce the Sushi Validator Archive Helm Chart, which can be used to deploy a production-ready instance of the Archive Node with Rosetta to a Kubernetes cluster of your choice. &lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Since I love Kubernetes, I am going to have a pretty opinionated setup, but I think &lt;em&gt;everyone&lt;/em&gt; should have a little understanding of how Containers and Kubernetes work in 2021. Fortunately, O(1) Labs has provided some Helm Charts that offer a good starting place when building a Mina deployment. &lt;/p&gt;
&lt;p&gt;You will need the following things in order to fully follow along with this post: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A kubernetes cluster running and &lt;code&gt;kubectl&lt;/code&gt; configured (I use the Kubernetes feature on Docker Desktop)&lt;/li&gt;
&lt;li&gt;Helm 3 installed &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It also might help to at least skim through the last post in the &lt;em&gt;March to Mainnet&lt;/em&gt; Series &lt;a href="https://connerswann.me/2021/02/mina-archive-node-kubernetes.html"&gt;about the Archive Node Helm Chart&lt;/a&gt;, which sets the scene for all the concepts we will cover in this post. &lt;/p&gt;
&lt;p&gt;A last note: The word "ledger" is incredibly overloaded in the crypto world. In this post I try to differentiate between the company/Hardware Wallet &lt;code&gt;Ledger&lt;/code&gt; (ex. Ledger Nano S) and the generic word &lt;code&gt;ledger&lt;/code&gt; (ex. genesis ledger) which describes a list of accounts in a blockchain system. &lt;/p&gt;
&lt;h2&gt;Install Mina Ledger App&lt;/h2&gt;
&lt;p&gt;If you have an address in the Mina Genesis Ledger, you should have already done this step and generated your address using the &lt;code&gt;mina_ledger_wallet&lt;/code&gt; utility. However, instructions for installing it can be &lt;a href="https://minaprotocol.com/docs/keypair/ledger-app-mina"&gt;found here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I am assuming that you have placed the &lt;code&gt;mina_ledger_wallet&lt;/code&gt; program somewhere in your &lt;code&gt;$PATH&lt;/code&gt;, and if you don't the commands you might use will be slightly different than those in this post. &lt;/p&gt;
&lt;p&gt;If you have done your due dilligence, you might have run &lt;code&gt;mina_ledger_wallet test-transaction&lt;/code&gt; which creates and signs a dummy transaction offline. This only requires the Ledger to preform successfully. &lt;/p&gt;
&lt;p&gt;If you were curious and tried to run &lt;code&gt;mina_ledger_wallet send-payment&lt;/code&gt; though, you might discover you get an error: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ mina_ledger_wallet --verbose send-payment --network testnet  --fee .01 &lt;span class="m"&gt;42&lt;/span&gt; &amp;lt;SenderKey&amp;gt; &amp;lt;ReceiverKey&amp;gt; .01
Getting network identifier... Error: HTTPConnectionPool&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;, &lt;span class="nv"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3087&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;: Max retries exceeded with url: /network/list &lt;span class="o"&gt;(&lt;/span&gt;Caused by NewConnectionError&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;lt;urllib3.connection.HTTPConnection object at 0x1105912b0&amp;gt;: Failed to establish a new connection: [Errno 61] Connection refused&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Question: What is the &lt;code&gt;mina_ledger_wallet&lt;/code&gt; program expecting to be running on port &lt;code&gt;3087&lt;/code&gt;? The Mina Graphql Port binds to port &lt;code&gt;3085&lt;/code&gt; by default...&lt;/p&gt;
&lt;p&gt;Answer: &lt;code&gt;mina_ledger_wallet&lt;/code&gt; is looking for the &lt;code&gt;mina-rosetta&lt;/code&gt; API! &lt;/p&gt;
&lt;p&gt;This actually makes sense, &lt;code&gt;mina_ledger_wallet&lt;/code&gt; needs up-to-date information from the blockchain about your account in order to properly format the transaction. Lets get that set up!&lt;/p&gt;
&lt;h2&gt;Run an Archive Node with Rosetta&lt;/h2&gt;
&lt;p&gt;In the &lt;a href="https://connerswann.me/2021/02/mina-archive-node-kubernetes.html"&gt;last post&lt;/a&gt;, we discussed using the official Archive Node Helm Chart to install an Archive Node in Kubernetes. The official chart is designed for testnets, works great for development, and archives blocks no problem. However, to send transactions with the Ledger Nano S, we need the Rosetta API too! &lt;/p&gt;
&lt;p&gt;First, an aside, what &lt;em&gt;exactly&lt;/em&gt; is Rosetta? &lt;/p&gt;
&lt;p&gt;From the &lt;a href="https://www.rosetta-api.org/"&gt;rosetta-api website&lt;/a&gt;: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Rosetta is an open standard designed to simplify blockchain deployment and interaction. Spend less time on integration and more time on novel blockchain advancements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Mina Rosetta Diagram" src="../../images/2021/mina-rosetta-architecture.png"&gt;&lt;/p&gt;
&lt;p&gt;Rosetta provides a nice standard API for every blockchain, which can be a pain from the perspective of one writing a blockchain, but it is &lt;em&gt;hugely&lt;/em&gt; useful when a big organization just forces everyone to adhere to the &lt;em&gt;same API&lt;/em&gt;. Rosetta runs as a sidecar to the Archive Node deployment, consuming the GraphQL Port on the Daemon as well as data from the Postgres Database directly. &lt;/p&gt;
&lt;/hr&gt;
&lt;p&gt;In order to facilitate the deployment of Rosetta, and add some more production-oriented features, we at the Sushi Validator forked the official chart and are hosting it on the official &lt;a href="https://minastakingpool.connerswann.me/"&gt;Sushi Validator&lt;/a&gt; helm repository &lt;a href="https://github.com/Sushi-Validator/helm-charts"&gt;here&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;The &lt;code&gt;values.yaml&lt;/code&gt; is compatible with the official chart, and is a super-set of the existing fields. Most notably, a &lt;code&gt;rosetta&lt;/code&gt; section was added: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-yaml"&gt;
rosetta: 
  image: gcr.io/o1labs-192920/coda-daemon-baked:0.4.2-245a3f7-zenith-7a89538
  graphqlUri: http://127.0.0.1:3085/graphql 
  archiveUri: '{{ $.Values.archive.postgresUri }}'
  ports:
    web: 3087
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Note: Fortunately O(1) packages the &lt;code&gt;mina-rosetta&lt;/code&gt; binary in the &lt;code&gt;coda-daemon-baked&lt;/code&gt; image, so we can use the same exact image as the Mina Daemon here.&lt;/p&gt;
&lt;p&gt;A complete example &lt;code&gt;values.yaml&lt;/code&gt; follows: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-yaml"&gt;
testnetName: "zenith"
coda:
  runtimeConfig: ""
  logLevel: "Info"
  logSnarkWorkGossip: false
  image: gcr.io/o1labs-192920/coda-daemon-baked:0.4.2-245a3f7-zenith-7a89538
  privkeyPass: "naughty blue worm"
  seedPeers:
    - /dns4/seed-1.zenith.o1test.net/tcp/10000/p2p/12D3KooWEEkNQY482QZ9RzTjsAYnczNNWS592guYKZHn9MMAkqpj
    - /dns4/mina-seed-1.zkvalidator.com/tcp/8302/p2p/12D3KooWSR7LMBSfEk3LQUudmsX27yuRHe9NUxwLumurGF5P1MNS
    - /dns4/mina-1.figment.io/tcp/8302/p2p/12D3KooWSkfwArLtqGMht1a9w3z3QiiqA2E6seBRAk378rvanGRZ
  ports:
    client: "8301"
    graphql: "3085"
    metrics: "10001"
    p2p: "10909"

archive:
  hostPort: "10909"
  image: gcr.io/o1labs-192920/coda-archive:0.4.2-245a3f7
  listenPort: "3086"
  nodeName: "dev"
  remoteSchemaFile: "https://raw.githubusercontent.com/MinaProtocol/mina/develop/src/app/archive/create_schema.sql"
  postgresHost: '{{ .Release.Name }}-postgresql'
  postgresPort: "5432"
  postgresDB: "archive"
  postgresUri: postgres://{{ .Values.postgresql.postgresqlUsername }}:{{ .Values.postgresql.postgresqlPassword }}@{{ tpl .Values.archive.postgresHost . }}:{{ .Values.archive.postgresPort }}/{{ .Values.archive.postgresDB }}
  ports:
    server: 3086
    postgres: "5432"

postgresql:
  postgresqlPassword: "foobar"
  postgresqlUsername: "postgres"

rosetta: 
  image: gcr.io/o1labs-192920/coda-daemon-baked:0.4.2-245a3f7-zenith-7a89538
  graphqlUri: http://127.0.0.1:3085/graphql 
  archiveUri: '{{ $.Values.archive.postgresUri }}'
  ports:
    web: 3087

healthcheck:
  enabled: true
  failureThreshold: 60
  periodSeconds: 5
  initialDelaySeconds: 30

nodeSelector:
  preemptible: false
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Now, given this &lt;code&gt;values.yaml&lt;/code&gt;, all we need to do now is add the Sushi Validator helm repository: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
helm repo add sushi https://charts.sushivalidator.com
helm repo update
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;We can install the chart like so: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
helm install --values values.yaml archive sushi/archive-node/
NAME: archive
LAST DEPLOYED: Sun Feb 22 19:03:12 2021
NAMESPACE: archive
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Once installed, we'll get a deployment that looks like this: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Mina Archive Helm Chart" src="../../images/2021/archive-node-helm-chart.png"&gt;&lt;/p&gt;
&lt;p&gt;We can check the deployment by viewing the currently running pods: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ kubectl get pods
NAME                         READY   STATUS      RESTARTS   AGE
archive-5d9b967557-pkk2g     3/3     Running     0          37s
archive-db-bootstrap-25mml   0/3     Completed   0          37s
archive-db-bootstrap-nhp7f   0/3     Error       0          25s
archive-db-bootstrap-nx9rw   0/3     Error       0          15s
archive-postgresql-0         1/1     Running     0          37s
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Note: Depending on how long it takes for the Postgres container to pull and start, you might see one or more failed &lt;code&gt;archive-db-bootstrap-xxxxx&lt;/code&gt; jobs, this is normal. &lt;/p&gt;
&lt;p&gt;You can check your daemon's sync status with the following command: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl exec $(pods | grep archive- | head -n 1 | awk '{print $1}') -c coda -- coda client status&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should get output that looks similar to this: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ kubectl exec $(pods | grep archive- | head -n 1 | awk '{print $1}') -c coda -- coda client status
Coda daemon status
-----------------------------------

Max observed block height:              932
Max observed unvalidated block height:  0
Local uptime:                           2m42s
Chain id:                               394692fea7f6531810be6ef213959856010881425920d396be39009d53045074
Git SHA-1:                              [DIRTY]245a3f7d883c516f5f16742cb1ca672872612851
Configuration directory:                /root/.coda-config
Peers:                                  33
User_commands sent:                     0
SNARK worker:                           None
SNARK work fee:                         100000000
Sync status:                            Bootstrap
Block producers running:                0
Consensus time now:                     epoch=0, slot=2076
Consensus mechanism:                    proof_of_stake
Consensus configuration:                
        Delta:                     0
        k:                         290
        Slots per epoch:           7140
        Slot duration:             3m
        Epoch duration:            14d21h
        Chain start timestamp:     2021-02-17 19:30:00.000000Z
        Acceptable network delay:  3m

Addresses and ports:                    
        External IP:    &lt;IP ADDRESS&gt;
        Bind IP:        0.0.0.0
        Libp2p PeerID:  12D3KooWSfZd2tcaB5oPbu34KzcREoN8BNhQLofmSDguG3dRXzPS
        Libp2p port:    10909
        Client port:    8301
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Eventually, after ~10 minutes or so, your Daemon will be &lt;code&gt;Synced&lt;/code&gt; and chain data will be flowing to your Postgres database. &lt;/p&gt;
&lt;h2&gt;A Little About Rosetta&lt;/h2&gt;
&lt;p&gt;Rosetta is an initiative from Coinbase, providing a standard API for applications built on top of blockchains to interact with the underlying ledger. It is a robust and well-tested interface, and it's pretty nice to work with. I think it would be fun to write some posts about writing applications on top of &lt;code&gt;mina-rosetta&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you want to poke around, there is a cool rosetta-api client which can be found &lt;a href="https://github.com/coinbase/rosetta-cli"&gt;on github, here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can port-forward the Rosetta API Port so the CLI can communicate with the remote pod:&lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
kubectl port-forward $(pods | grep archive- | head -n 1 | awk '{print $1}')  3087
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Next, we need to configure the &lt;code&gt;rosetta-cli&lt;/code&gt; with a &lt;code&gt;rosetta.conf&lt;/code&gt; -- an example &lt;a href="https://gist.github.com/yourbuddyconner/89837c85b409439bdd07acdd37425e34"&gt;can be found here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now we can do things like query blocks: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ rosetta-cli --configuration-file rosetta.conf view:block 1
loaded configuration file: rosetta.conf

Current Block:
{
 "block_identifier": {
  "index": 1,
  "hash": "3NLH8GM4afTu4UdbE6nD5yELa5yV78PzdN8QTYYXrL7g1xoPhBGw"
 },
 "parent_block_identifier": {
  "index": 1,
  "hash": "3NLH8GM4afTu4UdbE6nD5yELa5yV78PzdN8QTYYXrL7g1xoPhBGw"
 },
 "timestamp": 0,
 "transactions": [],
 "metadata": {
  "creator": "B62qiy32p8kAKnny8ZFwoMhYpBppM1DWVCqAPBYNcXnsAHhnfAAuXgg"
 }
}
Balance Changes:
Cummulative: 3NLH8GM4afTu4UdbE6nD5yELa5yV78PzdN8QTYYXrL7g1xoPhBGw

Operation Groups:
&lt;/code&gt;
&lt;/pre&gt;

&lt;h2&gt;mina_ledger_wallet CLI&lt;/h2&gt;
&lt;p&gt;Now for the reason we are all here, lets send some transactions. Now that we have a Rosetta API running, and port-forwarded (if you didn't do this, go do it now!), we can  now use the &lt;code&gt;mina_ledger_wallet&lt;/code&gt; CLI to send a transaction. &lt;/p&gt;
&lt;p&gt;Lets look at the help output: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ mina_ledger_wallet send-payment -h 
usage: mina_ledger_wallet send-payment [-h] [--mina_url MINA_URL] [--network NETWORK] [--fee FEE] [--nonce NONCE] [--valid_until VALID_UNTIL] [--memo MEMO]
                                       sender_bip44_account sender_address receiver amount

positional arguments:
  sender_bip44_account  BIP44 account to send from (e.g. 42)
  sender_address        Mina address of sender
  receiver              Mina address of recipient
  amount                Payment amount you want to send

optional arguments:
  -h, --help            show this help message and exit
  --mina_url MINA_URL   Mina rosetta interface url (default http://localhost:3087)
  --network NETWORK     Network override
  --fee FEE             Fee override
  --nonce NONCE         Nonce override
  --valid_until VALID_UNTIL
                        Valid until
  --memo MEMO           Transaction memo (publicly visible)
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;So, a properly formatted command sending &lt;code&gt;0.1&lt;/code&gt; Mina from &lt;code&gt;&amp;lt;SenderAddress&amp;gt;&lt;/code&gt; to &lt;code&gt;&amp;lt;ReceiverAddress&amp;gt;&lt;/code&gt; would look something like: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
mina_ledger_wallet \
    send-payment \
    --network testnet \
    --mina_url http://localhost:3087 \
    --fee .01 \
    42 \
    SenderAddress \
    ReceiverAddress \
    0.1
&lt;/code&gt;
&lt;/pre&gt;

&lt;h2&gt;Send a Transaction&lt;/h2&gt;
&lt;p&gt;Lets do it! &lt;/p&gt;
&lt;p&gt;Here's some example output from that command: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ mina_ledger_wallet --verbose send-payment --network testnet --mina_url http://localhost:3087 --fee .01 42 B62qqYpdvJ8oUF81gZL1WfLef1Ky8SsdvmUyfpr7YWb2m2rjRxaEcxq B62qmdut1usji2NFZSn9BrywhjHakYtAvL7Mdy7muyjcbrY3hKJ688Y .01
Using rosetta override: http://localhost:3087
Getting network identifier... debug
Using network override: testnet
Getting account nonce and suggested fee... done
Using fee override: .01
Using nonce override: 2
Getting account balance... done

Sign transaction:
    Type:        Payment
    Account:     42 (path 44'/12586'/42'/0/0)
    Sender:      B62qqYpdvJ8oUF81gZL1WfLef1Ky8SsdvmUyfpr7YWb2m2rjRxaEcxq (balance 0.98)
    Receiver:    B62qmdut1usji2NFZSn9BrywhjHakYtAvL7Mdy7muyjcbrY3hKJ688Y
    Amount:      0.010000000
    Fee:         0.010000000
    Total:       0.020000000
    Nonce:       2

Continue? (y/N) y

Constructing unsigned payment transaction... done

UNSIGNED_TX = {"randomOracleInput": "", "payment": {"to": "B62qmdut1usji2NFZSn9BrywhjHakYtAvL7Mdy7muyjcbrY3hKJ688Y", "from": "B62qqYpdvJ8oUF81gZL1WfLef1Ky8SsdvmUyfpr7YWb2m2rjRxaEcxq", "fee": "10000000", "token": "1", "nonce": "2", "memo": null, "amount": "10000000", "valid_until": null}, "stakeDelegation": null, "createToken": null, "createTokenAccount": null, "mintTokens": null}

Signing transaction (please confirm on Ledger device)... 
apduMessage hex (346) = &lt;HEX_OUTPUT&gt;

SIGNED_TX   = {"signature": "1f0f4ef044b259a1851cfffc98307beef4b388047dcf57de53debdce83fa2d661e1dc6db0b2c14aef2cd6227682ac5376f5561a741b7f6338ea55245e190eea8", "payment": {"to": "B62qmdut1usji2NFZSn9BrywhjHakYtAvL7Mdy7muyjcbrY3hKJ688Y", "from": "B62qqYpdvJ8oUF81gZL1WfLef1Ky8SsdvmUyfpr7YWb2m2rjRxaEcxq", "fee": "10000000", "token": "1", "nonce": "2", "memo": null, "amount": "10000000", "valid_until": "4294967295"}, "stake_delegation": null, "create_token": null, "create_token_account": null, "mint_tokens": null}


Send transaction:
    Type:        Payment
    Account:     42 (path 44'/12586'/42'/0/0)
    Sender:      B62qqYpdvJ8oUF81gZL1WfLef1Ky8SsdvmUyfpr7YWb2m2rjRxaEcxq (balance 0.98)
    Receiver:    B62qmdut1usji2NFZSn9BrywhjHakYtAvL7Mdy7muyjcbrY3hKJ688Y
    Amount:      0.010000000
    Fee:         0.010000000
    Total:       0.020000000
    Nonce:       2
    Signature:   1f0f4ef044b259a1851cfffc98307beef4b388047dcf57de53debdce83fa2d661e1dc6db0b...

Continue? (y/N)
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;I didn't actually send this test transaction, but you totally can and should!&lt;/p&gt;
&lt;p&gt;You can check your balance on MinaExplorer.com by plugging your sender or receiver address into the following URL and visiting it in your browser: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;https://minaexplorer.com/wallet/&amp;lt;ADDRESS&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;With any luck, you should see something like this: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Mina Transaction" src="../../images/2021/ledger-transaction-screenshot.png"&gt;&lt;/p&gt;</content><category term="Mina Protocol"></category><category term="Mina"></category><category term="Blockchain"></category><category term="Archive Node"></category></entry><entry><title>March to Mainnet: Running a Mina Archive Node in Kubernetes</title><link href="https://connerswann.me/2021/02/mina-archive-node-kubernetes.html" rel="alternate"></link><published>2021-02-21T00:00:00-08:00</published><updated>2021-02-21T00:00:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2021-02-21:/2021/02/mina-archive-node-kubernetes.html</id><summary type="html">&lt;h2&gt;Mina Protocol&lt;/h2&gt;
&lt;p&gt;For the uninitiated, Mina Protocol is a new crypto network. Mina implements a "succinct" blockchain that effectively "&lt;a href="https://minaprotocol.com/"&gt;swaps the traditional blockchain for a tiny cryptographic proof.&lt;/a&gt;" &lt;/p&gt;
&lt;p&gt;Practically, this means that in the average Mina node, once a block with transactions is "swapped" for a proof, it gets thrown …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Mina Protocol&lt;/h2&gt;
&lt;p&gt;For the uninitiated, Mina Protocol is a new crypto network. Mina implements a "succinct" blockchain that effectively "&lt;a href="https://minaprotocol.com/"&gt;swaps the traditional blockchain for a tiny cryptographic proof.&lt;/a&gt;" &lt;/p&gt;
&lt;p&gt;Practically, this means that in the average Mina node, once a block with transactions is "swapped" for a proof, it gets thrown out to save disk space. &lt;/p&gt;
&lt;p&gt;Earlier blockchain implementations like Bitcoin and Ethereum don't have this ability, and as such the hardware requirements for running a Node on their networks increase steadily with the size of their respective chains (240GB and &amp;gt;1TB respectively).&lt;/p&gt;
&lt;p&gt;However, due to the succinct property of the Mina blockchain, an archive node is needed to store chain data that is SNARKed away. Whether it be for tax purposes, to calculate staking payouts, or just to have access to the data, numerous organizations will need to run archive nodes, so I decided to spend some time documenting the process for those that will inevitably follow after. &lt;/p&gt;
&lt;p&gt;My approach is a little opinionated, but it derives best-practices from the cutting edge of container orchestration and cloud computing. I definitely recommend this approach, however be aware that this definitely not the only way to cook the sausage here. &lt;/p&gt;
&lt;h2&gt;Technology and Prerequesites&lt;/h2&gt;
&lt;p&gt;I like Kubernetes. Fortunately, O(1) Labs has provided some handy &lt;a href="https://github.com/MinaProtocol/mina/tree/develop/helm"&gt;Helm Charts&lt;/a&gt; that make our collective lives pretty easy and serve as a starting point to build an archive node deployment off of. &lt;/p&gt;
&lt;p&gt;In this post we will focus on deploying to a local Minikube/Docker Desktop cluster, but this will definitely work on any Kubernetes cluster (the &lt;em&gt;joy&lt;/em&gt; of Kubernetes!). &lt;/p&gt;
&lt;p&gt;I assume you have the following things set up: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A local Kubernetes cluster running and &lt;code&gt;kubectl&lt;/code&gt; configured. &lt;/li&gt;
&lt;li&gt;The Helm 3 CLI. &lt;/li&gt;
&lt;li&gt;PostgreSQL installed locally if you would like to make queries. &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Archive Node Helm Chart&lt;/h2&gt;
&lt;p&gt;The Archive Node Helm Chart lives in the Mina Monorepo and can be used to deploy a moderately-resilient Archive process that indexes blocks off the blockchain. It is perfectly suitable for testnets or local development. Since O(1) has a Helm Repo, you can simply install the Repo, provide a &lt;code&gt;values.yaml&lt;/code&gt; to override the default values, and deploy! &lt;/p&gt;
&lt;p&gt;The chart consists of three discrete processes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mina Daemon - The star of the show, syncs with the blockchain and does all the hard work&lt;/li&gt;
&lt;li&gt;Archive Process - Receives RPC Updates from the Mina Daemon and persists on-chain data to the Postgres DB&lt;/li&gt;
&lt;li&gt;PostgreSQL Database - Stores Archive data (via bitnami sub-chart)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Mina Archive Node Architecture" src="../../images/2021/mina-archive-diagram-1.png"&gt;&lt;/p&gt;
&lt;h3&gt;Install the Helm Repo&lt;/h3&gt;
&lt;p&gt;The instructions for installing the Mina Helm Repo can be found &lt;a href="https://github.com/MinaProtocol/mina/tree/develop/helm/archive-node"&gt;here&lt;/a&gt; and can be found below. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;helm repo add mina https://coda-charts.storage.googleapis.com
helm repo update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Helm Chart Values&lt;/h3&gt;
&lt;p&gt;The default values for the helm chart can be found in the repo &lt;a href="https://github.com/MinaProtocol/mina/blob/develop/helm/archive-node/values.yaml"&gt;here&lt;/a&gt;. Simply create a yaml file locally that overrides one or more of them like so: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-yaml"&gt;
testnetName: "zenith"
coda:
  runtimeConfig: ""
  logLevel: "Info"
  logSnarkWorkGossip: false
  image: gcr.io/o1labs-192920/coda-daemon-baked:0.4.2-245a3f7-zenith-7a89538
  privkeyPass: "naughty blue worm"
  seedPeers:
    - /dns4/seed-1.zenith.o1test.net/tcp/10000/p2p/12D3KooWEEkNQY482QZ9RzTjsAYnczNNWS592guYKZHn9MMAkqpj
    - /dns4/mina-seed-1.zkvalidator.com/tcp/8302/p2p/12D3KooWSR7LMBSfEk3LQUudmsX27yuRHe9NUxwLumurGF5P1MNS
    - /dns4/mina-1.figment.io/tcp/8302/p2p/12D3KooWSkfwArLtqGMht1a9w3z3QiiqA2E6seBRAk378rvanGRZ
  ports:
    client: "8301"
    graphql: "3085"
    metrics: "10001"
    p2p: "10909"

archive:
  hostPort: "10909"
  image: gcr.io/o1labs-192920/coda-archive:0.4.2-245a3f7
  listenPort: "3086"
  nodeName: "dev"
  remoteSchemaFile: "https://raw.githubusercontent.com/MinaProtocol/mina/develop/src/app/archive/create_schema.sql"
  postgresHost: '{{ .Release.Name }}-postgresql'
  postgresPort: "5432"
  postgresDB: "archive"
  postgresUri: postgres://{{ .Values.postgresql.postgresqlUsername }}:{{ .Values.postgresql.postgresqlPassword }}@{{ tpl .Values.archive.postgresHost . }}:{{ .Values.archive.postgresPort }}/{{ .Values.archive.postgresDB }}
  ports:
    server: 3086
    postgres: "5432"

postgresql:
  postgresqlPassword: "foobar"
  postgresqlUsername: "postgres"

healthcheck:
  enabled: true
  failureThreshold: 60
  periodSeconds: 5
  initialDelaySeconds: 30

nodeSelector:
  preemptible: false
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;There's a couple pieces here that we should examine closely. &lt;/p&gt;
&lt;p&gt;This section is for Mina (previously known as Coda) Daemon configuration variables. I have provided a recently built image (official images can be found &lt;a href="gcr.io/o1labs-192920/coda-daemon-baked"&gt;here&lt;/a&gt;), and valid seed peers for the network to which I would like to connect.&lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-yaml"&gt;
coda:
  runtimeConfig: ""
  logLevel: "Info"
  logSnarkWorkGossip: false
  image: gcr.io/o1labs-192920/coda-daemon-baked:0.4.2-245a3f7-zenith-7a89538
  seedPeers:
    - /dns4/seed-1.zenith.o1test.net/tcp/10000/p2p/12D3KooWEEkNQY482QZ9RzTjsAYnczNNWS592guYKZHn9MMAkqpj
    - /dns4/mina-seed-1.zkvalidator.com/tcp/8302/p2p/12D3KooWSR7LMBSfEk3LQUudmsX27yuRHe9NUxwLumurGF5P1MNS
    - /dns4/mina-1.figment.io/tcp/8302/p2p/12D3KooWSkfwArLtqGMht1a9w3z3QiiqA2E6seBRAk378rvanGRZ
  ports:
    client: "8301"
    graphql: "3085"
    metrics: "10001"
    p2p: "10909"
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;This section configures the Archive Node, and we have overriden the Docker Image. It is worth calling out the &lt;code&gt;archive.remoteSchemaFile&lt;/code&gt; value, which can be used to pass in the remote SQL file that will bootstrap the Database. &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-yaml"&gt;
archive:
  hostPort: "10909"
  image: gcr.io/o1labs-192920/coda-archive:0.4.2-245a3f7
  listenPort: "3086"
  nodeName: "dev"
  remoteSchemaFile: "https://raw.githubusercontent.com/MinaProtocol/mina/develop/src/app/archive/create_schema.sql"
  postgresHost: '{{ .Release.Name }}-postgresql'
  postgresPort: "5432"
  postgresDB: "archive"
  postgresUri: postgres://{{ .Values.postgresql.postgresqlUsername }}:{{ .Values.postgresql.postgresqlPassword }}@{{ tpl .Values.archive.postgresHost . }}:{{ .Values.archive.postgresPort }}/{{ .Values.archive.postgresDB }}
  ports:
    server: 3086
    postgres: "5432"
&lt;/code&gt;
&lt;/pre&gt;

&lt;h2&gt;Deploy it!&lt;/h2&gt;
&lt;p&gt;So, now that we have a &lt;code&gt;values.yaml&lt;/code&gt; file locally, and assuming we have installed the Mina Helm Repo correctly, we can do the following: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ helm install --values values.yaml archive mina/archive-node
NAME: archive
LAST DEPLOYED: Sun Feb 21 19:03:12 2021
NAMESPACE: archive
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;We can check the deployment by viewing the currently running pods: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ kubectl get pods
NAME                         READY   STATUS      RESTARTS   AGE
archive-5d9b967557-pkk2g     2/2     Running     0          37s
archive-db-bootstrap-25mml   0/3     Completed   0          37s
archive-db-bootstrap-nhp7f   0/3     Error       0          25s
archive-db-bootstrap-nx9rw   0/3     Error       0          15s
archive-postgresql-0         1/1     Running     0          37s
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Note: Depending on how long it takes for the Postgres container to pull and start, you might see one or more failed &lt;code&gt;archive-db-bootstrap-xxxxx&lt;/code&gt; jobs, this is normal. &lt;/p&gt;
&lt;p&gt;You can check your daemon's sync status with the following command: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl exec $(pods | grep archive- | head -n 1 | awk '{print $1}') -c coda -- coda client status&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should get output that looks similar to this: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ kubectl exec $(pods | grep archive- | head -n 1 | awk '{print $1}') -c coda -- coda client status
Coda daemon status
-----------------------------------

Max observed block height:              932
Max observed unvalidated block height:  0
Local uptime:                           2m42s
Chain id:                               394692fea7f6531810be6ef213959856010881425920d396be39009d53045074
Git SHA-1:                              [DIRTY]245a3f7d883c516f5f16742cb1ca672872612851
Configuration directory:                /root/.coda-config
Peers:                                  33
User_commands sent:                     0
SNARK worker:                           None
SNARK work fee:                         100000000
Sync status:                            Bootstrap
Block producers running:                0
Consensus time now:                     epoch=0, slot=2076
Consensus mechanism:                    proof_of_stake
Consensus configuration:                
        Delta:                     0
        k:                         290
        Slots per epoch:           7140
        Slot duration:             3m
        Epoch duration:            14d21h
        Chain start timestamp:     2021-02-17 19:30:00.000000Z
        Acceptable network delay:  3m

Addresses and ports:                    
        External IP:    &lt;IP ADDRESS&gt;
        Bind IP:        0.0.0.0
        Libp2p PeerID:  12D3KooWSfZd2tcaB5oPbu34KzcREoN8BNhQLofmSDguG3dRXzPS
        Libp2p port:    10909
        Client port:    8301
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Eventually, after ~10 minutes or so, your Daemon will be &lt;code&gt;Synced&lt;/code&gt; and chain data will be flowing to your Postgres database. &lt;/p&gt;
&lt;h2&gt;Ok, now what?&lt;/h2&gt;
&lt;p&gt;Now, you have a working, running Archive Node! Lets run a test query to see whats in the database. &lt;/p&gt;
&lt;p&gt;First, we should forward the Postgres port from the Database container to &lt;code&gt;localhost&lt;/code&gt; with &lt;code&gt;kubectl&lt;/code&gt;: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ k port-forward archive-postgresql-0  5432
Forwarding from 127.0.0.1:5432 -&gt; 5432
Forwarding from [::1]:5432 -&gt; 5432
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;In another terminal, connect to the database with the &lt;code&gt;psql&lt;/code&gt; client using the credentials we set in the &lt;code&gt;values.yaml&lt;/code&gt; above: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ psql -h localhost --user postgres
Password for user postgres: 
psql (13.2, server 11.10)
Type "help" for help.

postgres=# 
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;We can list the databases, switch to the archive database, list the tables, and query one of them: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-shell"&gt;
$ psql -h localhost --user postgres
Password for user postgres: 
psql (13.2, server 11.10)
Type "help" for help.

postgres=# \l
                                  List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges   
-----------+----------+----------+-------------+-------------+-----------------------
 archive   | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
(4 rows)

postgres=# \c archive 
psql (13.2, server 11.10)
You are now connected to database "archive" as user "postgres".
archive=# \dt
                  List of relations
 Schema |           Name           | Type  |  Owner   
--------+--------------------------+-------+----------
 public | balances                 | table | postgres
 public | blocks                   | table | postgres
 public | blocks_internal_commands | table | postgres
 public | blocks_user_commands     | table | postgres
 public | epoch_data               | table | postgres
 public | internal_commands        | table | postgres
 public | public_keys              | table | postgres
 public | snarked_ledger_hashes    | table | postgres
 public | timing_info              | table | postgres
 public | user_commands            | table | postgres
(10 rows)

archive=# SELECT id, state_hash, parent_id, parent_hash, creator_id, height, global_slot FROM BLOCKS LIMIT 5;
 id |                      state_hash                      | parent_id |                     parent_hash                      | creator_id | height | global_slot 
----+------------------------------------------------------+-----------+------------------------------------------------------+------------+--------+-------------
  1 | 3NL4SSDnG5EbFBV8kj5J3fmwuABQzUBrybVEHwYiGmudJUJMAFkc |           | 3NLr2QiYhu2mp4GhzEmVty1yWpW7QkkEjrkYJUrzBHgLRvDNL1nY |          1 |    642 |        1537
  2 | 3NKqnVZJYHWHmsNujM21HHDt2U7PVDLsu7AxnTq1uzjNKw1Rx6iC |         1 | 3NL4SSDnG5EbFBV8kj5J3fmwuABQzUBrybVEHwYiGmudJUJMAFkc |          1 |    643 |        1538
  3 | 3NLnmcUMRF9TproRxydkwqvtjYQjzNw59koQ62GYb4MRHKuQj1P1 |         2 | 3NKqnVZJYHWHmsNujM21HHDt2U7PVDLsu7AxnTq1uzjNKw1Rx6iC |          6 |    644 |        1539
  4 | 3NKZ1uZPPqssNtKkvwN3gDgw6vsuWjSDervuSd8QBEy2nswwgc4w |         3 | 3NLnmcUMRF9TproRxydkwqvtjYQjzNw59koQ62GYb4MRHKuQj1P1 |          9 |    645 |        1540
  5 | 3NKqcnzLzNScVMQf6xV1uBz1LnLTw9pedR8NfRtSa9tFkXxvbDqV |         4 | 3NKZ1uZPPqssNtKkvwN3gDgw6vsuWjSDervuSd8QBEy2nswwgc4w |         11 |    646 |        1542
(5 rows)
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Now, there are a few limitations here. For the moment, there is no way to retrieve missing blocks from the chain. As such, you will need to have a nice friend with a SQL dump of their database to fill in the gaps. However, this definitely provides a workable dev environment for those requiring access to a working Archive node. Good work around Archive node redundancy is being done, details can be found in this Docs commit &lt;a href="https://github.com/MinaProtocol/mina/blob/0351fc31e9f5e29dc87a04bb55f3cdc5aee2038e/docs/archive-redundancy.md"&gt;here&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;There is so much to do from here, like adding the &lt;code&gt;mina-rosetta&lt;/code&gt; server to expose data in a consistent format for the Ledger, Postgres database backups, and more! &lt;/p&gt;
&lt;p&gt;I invite you to try this out and stay tuned for more posts in this &lt;em&gt;March to Mainnet&lt;/em&gt; series. Next time, how to hook up this Archive deployment to your Ledger for fun and profit! &lt;/p&gt;</content><category term="Mina Protocol"></category><category term="Mina"></category><category term="Blockchain"></category><category term="Archive Node"></category></entry><entry><title>Weekend Project - Coda Blockchain Visualization</title><link href="https://connerswann.me/2019/09/weekend-project-coda-blockchain.html" rel="alternate"></link><published>2019-09-14T00:00:00-07:00</published><updated>2019-09-14T00:00:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2019-09-14:/2019/09/weekend-project-coda-blockchain.html</id><summary type="html">&lt;h1&gt;What's Coda?&lt;/h1&gt;
&lt;p&gt;Coda is a Crypto Protocol that I am working on as part of the O(1) Labs Team. Coda implements a "succinct" blockchain that effectively "&lt;a href="https://codaprotocol.com/"&gt;swaps the traditional blockchain for a tiny cryptographic proof.&lt;/a&gt;" &lt;/p&gt;
&lt;p&gt;Practically, this means that in the average Coda node, once a block with transactions …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;What's Coda?&lt;/h1&gt;
&lt;p&gt;Coda is a Crypto Protocol that I am working on as part of the O(1) Labs Team. Coda implements a "succinct" blockchain that effectively "&lt;a href="https://codaprotocol.com/"&gt;swaps the traditional blockchain for a tiny cryptographic proof.&lt;/a&gt;" &lt;/p&gt;
&lt;p&gt;Practically, this means that in the average Coda node, once a block with transactions is "swapped" for a proof, it gets thrown out to save disk space. &lt;/p&gt;
&lt;p&gt;Earlier blockchain implementations like Bitcoin and Ethereum don't have this ability, and as such the hardware requirements for running a Node on their networks increase steadily with the size of their respective chains (240GB and &amp;gt;1TB respectively). &lt;/p&gt;
&lt;h1&gt;The Problem&lt;/h1&gt;
&lt;p&gt;When I decided to sit down and get my roommate &lt;a href="https://github.com/reem"&gt;@reem&lt;/a&gt; to help me with this visualization, it was to try and solve one particular issue I'd been having. There wasn't a way to &lt;em&gt;at-a-glance&lt;/em&gt; verify that the network was forked!&lt;/p&gt;
&lt;p&gt;Put simply, a &lt;a href="https://en.wikipedia.org/wiki/Fork_(blockchain)"&gt;"&lt;em&gt;Fork&lt;/em&gt;"&lt;/a&gt; is what happens when a blockchain diverges into two or more paths forward. They can occur for a multitude of reasons:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;intentional forks&lt;/em&gt; which can be scheduled and relate to protocol upgrades or hotfixes to the blockchain&lt;/li&gt;
&lt;li&gt;&lt;em&gt;unintentional forks&lt;/em&gt; due to hardware outages, network partitions, or protocol bugs &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In most blockchain contexts, the consensus algorithm is designed to work forks out as a matter of course, but when they don't work out it's an indicator that something is wrong. In any protocol that is in active development, knowing an &lt;em&gt;unintentional fork&lt;/em&gt; is occurring and quickly identifying the root-cause has an incredible amount of utility from an observability perspective -- both to the protocol developers and any community members who might be participating on a network. &lt;/p&gt;
&lt;p&gt;Fortunately for me, John Wu (&lt;a href="https://github.com/wu-s-john"&gt;@wu-s-john&lt;/a&gt;), one of the O(1) protocol engineers, had just implemented basic block archive functionality that allows a node to store all blocks that it observes, along with all the associated metadata. Armed with this data, I figured we could build a nice visualization of the chain and any forks. &lt;/p&gt;
&lt;h1&gt;The Visualization&lt;/h1&gt;
&lt;p&gt;The blockchain visualization is implemented using &lt;a href="https://reactjs.org/"&gt;React&lt;/a&gt;, the &lt;a href="https://www.apollographql.com/"&gt;Apollo GraphQL Client&lt;/a&gt;, and &lt;a href="https://github.com/uber/react-vis-force"&gt;React-Vis-Force&lt;/a&gt; a library for building D3-based force-directed graphs. Since I had used this kind of graph in the past, I opted to stick with something I was comfortable with -- as I will mention later, this might not have been the best approach. &lt;/p&gt;
&lt;p&gt;A graph from a previous project:
&lt;img alt="Social Media Graph" src="https://connerswann.me/2019/09/weekend-project-1-facebook-graph.png"&gt;&lt;/p&gt;
&lt;p&gt;This is an animated gif of the visualization with roughly 400 blocks depicted:
&lt;img alt="Chain Visualization Gif" src="https://connerswann.me/2019/09/weekend-project-1-chain-visualization.gif"&gt;&lt;/p&gt;
&lt;p&gt;While it's not the most practical, I really like this visualization because it &lt;em&gt;feels&lt;/em&gt; organic somehow. Leaving it up on a TV or extra monitor has a lava lamp vibe that I enjoy -- it's art!&lt;/p&gt;
&lt;h1&gt;Lessons Learned&lt;/h1&gt;
&lt;p&gt;There's a bunch of stuff I would have done differently had I been designing this for deployment in a production setting: &lt;/p&gt;
&lt;h4&gt;Live Loading of New Blocks&lt;/h4&gt;
&lt;p&gt;The visualization is pretty cool, but it needs to be reloaded every 5-10 minutes or so in order to retrieve new blocks that might have been produced. I discovered to my dismay that React-Vis-Force doesn't allow you to dynamically add and remove nodes in the graph without reloading the whole thing. In order to get this working, I'd have to fork their library or craft my own D3-based react components. &lt;/p&gt;
&lt;h4&gt;Block Pagination&lt;/h4&gt;
&lt;p&gt;Due to the &lt;em&gt;Archive&lt;/em&gt; functionality overriding the normal behavior of the Coda Daemon, there can be a lot of observed blocks in the database. Additionally, the &lt;code&gt;blocks&lt;/code&gt; query in the Coda Daemon's GraphQL endpoint doesn't &lt;em&gt;actually&lt;/em&gt; support pagination at the time of writing, so loading the page gets slow as nBlocks increases. Though, even if it did support pagination, you'd still have to load all the blocks first due to the lack of live loading. &lt;/p&gt;
&lt;p&gt;This obviously is only an issue in the current Archive Node implementation, and there's an &lt;a href="https://github.com/CodaProtocol/coda/pull/2901"&gt;RFC&lt;/a&gt; in the works for a more robust Archive process. &lt;/p&gt;
&lt;h4&gt;Node Arrangement On Load&lt;/h4&gt;
&lt;p&gt;When there's a lot of nodes on the graph, it takes a really long time to work all the crossings out. I wished there was a way I could have laid out the nodes from left-right/top-bottom by their timestamp at render time. &lt;/p&gt;
&lt;h4&gt;CORS Proxy&lt;/h4&gt;
&lt;p&gt;Since the Coda Daemon is serving data on a different URI than my development server, &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS"&gt;CORS&lt;/a&gt; comes into play. Since the Daemon doesn't support CORS headers, I had to run it through a proxy that would add the appropriate headers for me. Luckily, there's plenty of packages that do this, and I opted to use &lt;a href="https://www.npmjs.com/package/local-cors-proxy"&gt;local-cors-proxy&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Archive Node Proxy&lt;/h4&gt;
&lt;p&gt;The Archive node's endpoint is &lt;em&gt;dangerous&lt;/em&gt; in that there is no authentication, so anyone with access could change the Daemon's internal state. As such, the visualization is only compatible with a Daemon running on the same machine. I would love to build out a GraphQL Proxy of some kind in order to mitigate this risk. &lt;/p&gt;
&lt;h4&gt;More Useful "Block Explorer" Functionality&lt;/h4&gt;
&lt;p&gt;I did implement some basic "block explorer" functionality, in that if you select a node a &lt;code&gt;div&lt;/code&gt; appears with the JSON data from the block. I would have liked to spend more time displaying this data on the graph itself, via colors, numerical badges, or otherwise. &lt;/p&gt;
&lt;h4&gt;Click and Drag Nodes&lt;/h4&gt;
&lt;p&gt;One other major feature that would make the visualization feel more interactive would obviously be draggable nodes on the graph. D3 has full support for interactive elements, but it's another thing that wasn't possible with the React-Vis-Force library. &lt;/p&gt;
&lt;h4&gt;Different Color Profiles&lt;/h4&gt;
&lt;p&gt;I would love to mess with different colorings on the graph &lt;em&gt;way&lt;/em&gt; more than I did here. This visualization can be super useful for visually debugging the blockchain by coloring the blocks based on number of transactions, snark jobs, fee transfers, etc. &lt;/p&gt;
&lt;h4&gt;Examples of Different Coloring:&lt;/h4&gt;
&lt;p&gt;Red -&amp;gt; Main Chain / Blue -&amp;gt; Leaf Nodes: 
&lt;img alt="colored-by-leaves" src="https://connerswann.me/2019/09/weekend-project-1-chain-vis-red-blue.png"&gt;&lt;/p&gt;
&lt;p&gt;Black -&amp;gt; No Txs / Redder -&amp;gt; More Txs
&lt;img alt="colored-by-transactions" src="https://connerswann.me/2019/09/weekend-project-1-chain-vis-red-black.png"&gt;&lt;/p&gt;
&lt;h1&gt;Think You Can Do Better? &lt;/br&gt; (I Bet You Can!)&lt;/h1&gt;
&lt;p&gt;I am confident that there are people out there who are better at Javascript than I am. This was done in a few days with little regard for practical deployment concerns. As such, the project is licensed under the &lt;a href="https://github.com/yourbuddyconner/coda-chain-visualization/blob/master/license"&gt;Apache License 2.0&lt;/a&gt;, which is a permissive license that allows for derivative works! &lt;/p&gt;
&lt;p&gt;Check out the repository &lt;a href="https://github.com/yourbuddyconner/coda-chain-visualization"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Coda Protocol is still &lt;em&gt;very much&lt;/em&gt; in active development, and the O(1) Team is constantly pushing out fun projects like this -- there's so many ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Have an idea for some other &lt;/em&gt;&lt;em&gt;cool thing&lt;/em&gt;&lt;em&gt; I haven't mentioned?&lt;/em&gt; 
&lt;/br&gt;Join the &lt;a href="http://bit.ly/CodaDiscord"&gt;Coda Community&lt;/a&gt;, and lets talk about it!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Want to learn how to operate a Node on the Coda Network?&lt;/em&gt; 
&lt;/br&gt;There are regular &lt;a href="https://codaprotocol.com/docs/coda-testnet/"&gt;Testnets&lt;/a&gt; that are designed to help us surface bugs and teach the community how to use Coda. Come join us!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Want to spend some time building something useful to the Coda Community?&lt;/em&gt; 
&lt;/br&gt;Check out the &lt;a href="https://github.com/CodaProtocol/coda-grants"&gt;Coda Grant Proposal Repository&lt;/a&gt; for a list of projects O(1) Labs is sponsoring. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lastly, if you decide to make something cool with this let me know on &lt;a href="https://twitter.com/yourbuddyconner"&gt;Twitter&lt;/a&gt; or the &lt;a href="http://bit.ly/CodaDiscord"&gt;Coda Discord Server&lt;/a&gt; -- you can find me with &lt;code&gt;@conner&lt;/code&gt;! &lt;/p&gt;</content><category term="Coda Protocol"></category><category term="Javascript"></category><category term="React"></category><category term="Coda"></category><category term="Blockchain"></category><category term="Block Explorer"></category></entry><entry><title>My Mac Keeps Dropping Wi-Fi - How I Fixed It</title><link href="https://connerswann.me/2017/11/my-mac-keeps-dropping-wi-fi-how-i-fixed-it.html" rel="alternate"></link><published>2017-11-14T00:00:00-08:00</published><updated>2017-11-14T00:00:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2017-11-14:/2017/11/my-mac-keeps-dropping-wi-fi-how-i-fixed-it.html</id><summary type="html">&lt;p&gt;I run a lot of things on my Desktop Mac Pro at home -- from basic stuff like a Plex media server to more custom projects like web servers or a VPN gateway. It's pretty important that these semi-critical services stay online as much as possible, however in furtherance of this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I run a lot of things on my Desktop Mac Pro at home -- from basic stuff like a Plex media server to more custom projects like web servers or a VPN gateway. It's pretty important that these semi-critical services stay online as much as possible, however in furtherance of this goal I ran into one major issue: the shape of my house. &lt;/p&gt;
&lt;p&gt;My computer is upstairs and the cable modem is ~200 feet away, through multiple floors, in the basement. I obviously have in-home Wi-Fi, but occasionally the Mac would disconnect from the network with no warning, and the only way to get it back on was to physically be there at the keyboard. Not Optimal! &lt;/p&gt;
&lt;p&gt;It's not a gigantic issue because simply sitting down, logging in, and cycling the Wi-Fi power is all that's needed to reconnect. However, this is really inconvenient if I'm not physically there but still want to consume the media on the machine -- so I set out to fix the problem for good. &lt;/p&gt;
&lt;p&gt;Luckily for me, Apple is really good about providing command-line control over hardware functionality in most circumstances, and Wi-Fi is no exception. There is a handy program on all OS X Machines called &lt;code&gt;networksetup&lt;/code&gt;. Here's an excerpt from the man page entry: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;NAME&lt;/span&gt;
     &lt;span class="n"&gt;networksetup&lt;/span&gt; &lt;span class="c1"&gt;-- configuration tool for network settings in System Preferences.&lt;/span&gt;

&lt;span class="n"&gt;DESCRIPTION&lt;/span&gt;
     &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;networksetup&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;used&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;configure&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="n"&gt;settings&lt;/span&gt; &lt;span class="n"&gt;typically&lt;/span&gt; &lt;span class="n"&gt;configured&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="k"&gt;System&lt;/span&gt; &lt;span class="n"&gt;Preferences&lt;/span&gt; &lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;  &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;networksetup&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt;
     &lt;span class="n"&gt;requires&lt;/span&gt; &lt;span class="k"&gt;at&lt;/span&gt; &lt;span class="n"&gt;least&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;admin&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;privileges&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Most&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="k"&gt;set&lt;/span&gt; &lt;span class="n"&gt;commands&lt;/span&gt; &lt;span class="n"&gt;require&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;root&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;privileges&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

     &lt;span class="k"&gt;Any&lt;/span&gt; &lt;span class="n"&gt;flag&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;takes&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;accept&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;-&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;place&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;indicate&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="k"&gt;read&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="k"&gt;stdin&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Knowing that &lt;code&gt;networksetup&lt;/code&gt; exists and how it works, I set out to whip up a little BASH script that can be run with cron and can check for internet connectivity and cycle the Wi-Fi if necessary. Below is what I came up with: &lt;/p&gt;
&lt;script src="https://gist.github.com/yourbuddyconner/22c37e5246b041fd5c6e4d1e89db0d34.js"&gt;&lt;/script&gt;

&lt;p&gt;After the script was written, it was trivial to add it to my crontab to be executed once every couple minutes. I also set up external monitoring Namecheap's Dynamic DNS service and UptimeRobot.com, but that's a topic for a future post. &lt;/p&gt;</content><category term="MacOS"></category><category term="Wi-Fi"></category><category term="cron"></category><category term="MacOS"></category></entry><entry><title>MyCI - A Look Into My Personal Cloud Environment</title><link href="https://connerswann.me/2017/05/myci-a-look-into-my-personal-cloud-environment.html" rel="alternate"></link><published>2017-05-04T20:14:00-07:00</published><updated>2017-05-04T20:14:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2017-05-04:/2017/05/myci-a-look-into-my-personal-cloud-environment.html</id><summary type="html">&lt;h1&gt;Why Do I Need a Personal Cloud Environment?&lt;/h1&gt;
&lt;p&gt;For a long time now, I have enjoyed doing personal software projects that teach me new skills, frameworks or even programming languages. I think that an engineer in a computer-science-related field should always be learning new technology and innovating. However, at times …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Why Do I Need a Personal Cloud Environment?&lt;/h1&gt;
&lt;p&gt;For a long time now, I have enjoyed doing personal software projects that teach me new skills, frameworks or even programming languages. I think that an engineer in a computer-science-related field should always be learning new technology and innovating. However, at times it can be a real struggle to get anything done quickly. When it comes to complex web-based systems there's so much that goes into learning a new language or framework that the entire process quickly becomes overwhelming. &lt;/p&gt;
&lt;p&gt;This is a common problem that many small companies and startups face, and it's one I have personally run into time and time again in my personal projects. Due to the layered nature of web-based communications, there's potentially a &lt;em&gt;lot&lt;/em&gt; involved when setting up a development environment. That overhead can at times discourage me from even starting a cool new project to begin with, and that's no fun!&lt;/p&gt;
&lt;p&gt;My training and experience as a DevOps Engineer however comes in handy in this situation, so I set out to solve this tricky problem for myself. &lt;/p&gt;
&lt;h2&gt;How Does It Help Me?&lt;/h2&gt;
&lt;p&gt;I tend to take the computer scientist's approach whenever possible, and this tricky infrastructure problem is no exception. The Computer Science approach is traditionally "work smarter, not harder," so in that vein my motto for infrastructure is the following: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"If you're doing something manually, you're doing something wrong."&lt;/p&gt;
&lt;p&gt;-Conner Swann&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Any good DevOps automation system acts like a 10x multiplier for efficiency to a talented software engineer. It allows them to automate the tedium of their day-to-day and spend more time on software issues, which not only increases developer happiness but the happiness of their managers.&lt;/p&gt;
&lt;p&gt;I designed my personal cloud infrastructure with this in mind. The system reacts to triggers in my software development cycle and kicks off complex jobs and workflows that I would have started manually, almost before I considered doing it myself. &lt;/p&gt;
&lt;h1&gt;What Is My Stack?&lt;/h1&gt;
&lt;h2&gt;Docker&lt;/h2&gt;
&lt;p&gt;I love Docker. The current trend of containerization in tech has provided an incredibly powerful tool and abstraction that software developers everywhere can leverage to drastically increase efficiency. I use Docker because it allows the Software Engineer in me ignore the underlying hardware and focus entirely on the interaction between my code and the dependencies inside the container. &lt;/p&gt;
&lt;p&gt;One of my biggest gripes when I started coding was the dreaded conflicting dependency on my development machine. It's entirely possible to have one project that requires Python 2.7 and another that requires Python 3. If you don't necessarily know your way around a package manager, or worse you don't have access to one, you might be in for a bad time. &lt;/p&gt;
&lt;p&gt;Docker saves you from this by compartmentalizing the application and all its dependencies in a special "box" that is logically separated from all the other "boxes" that share the resources of the host machine. In this way, it saves you a lot of the headache when trying to get two applications you wrote to play nicely together on one host.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://connerswann.me/images/2017/Docker-API-infographic-container-devops-nordic-apis.png"&gt;&lt;/p&gt;
&lt;h2&gt;Container Orchestration&lt;/h2&gt;
&lt;p&gt;Once you figure out how powerful Docker can be, you ask the next logical question: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"How can I run a bunch of Docker containers at once?" &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This doesn't necessarily have a concrete answer. &lt;/p&gt;
&lt;p&gt;You could use Google's &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt; or &lt;a href="http://mesos.apache.org/"&gt;Apache Mesos&lt;/a&gt;, however, if most of your Docker experience is with Docker-Compose in a development environment, learning one or the other can be a blocker to even getting your personal cloud setup! Both Kubernetes and Mesos are amazing, enterprise-grade container orchestration systems in their own right, but unless you have experience using them, it's overkill for your average free-time software developer.&lt;/p&gt;
&lt;div class="image-div" style="width: 400px;"&gt;
![]({static}/images/2017/SWARM.png)
&lt;/div&gt;

&lt;p&gt;You can use &lt;a href="https://docs.docker.com/engine/swarm/"&gt;Docker Swarm&lt;/a&gt;, however container management is entirely through the CLI. If you want a web-based UI to make your life easier, you're out of luck because the UI offering from Docker inc. is enterprise-only and &lt;a href="https://www.docker.com/pricing"&gt;&lt;em&gt;very&lt;/em&gt;&lt;/a&gt; expensive.  While Docker (the company) isn't helping out hackers wrangle their containers, it's worth mentioning that there are open-source alternatives like &lt;a href="http://portainer.io/"&gt;Portainer&lt;/a&gt; and &lt;a href="https://shipyard-project.com/"&gt;Shipyard&lt;/a&gt; which do provide web UIs for Docker Swarm. &lt;/p&gt;
&lt;p&gt;The solution I eventually decided on is a free and open-source container management and orchestration software named &lt;a href="http://rancher.com/"&gt;Rancher&lt;/a&gt;. Rancher checks all my boxes: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It's Docker-based and natively supports Docker-Compose for stack definition.&lt;/li&gt;
&lt;li&gt;It's free and open-source.&lt;/li&gt;
&lt;li&gt;It's easy to install and configure. &lt;/li&gt;
&lt;li&gt;It's backed by a wonderful community and core development team that are very responsive to issues and suggestions on Slack and at meetups.&lt;/li&gt;
&lt;li&gt;It has a web-based UI that I can use to visually manage my containers. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because Rancher lets me so easily launch and manage containers, I can easily run an arbitrary number of web-based services. It's trivial to add more Docker hosts to your environments, meaning I can start small and scale up when I need more resources.&lt;br&gt;
&lt;img alt="Rancher" src="https://connerswann.me/images/2017/Rancher-Logo-Final-300x180.png"&gt;&lt;/p&gt;
&lt;h2&gt;Continuous Integration and Deployment&lt;/h2&gt;
&lt;p&gt;Now that the infrastructure is set up and running, it's time to start thinking about how you're going to run your own software on it. I personally am really annoyed by repetitive tasks and strive to streamline my development cycle as much as much as possible. &lt;/p&gt;
&lt;p&gt;I've used &lt;a href="https://jenkins.io/"&gt;Jenkins&lt;/a&gt; extensively at work, and while he's a real workhorse he's a bit overkill for my purposes. Instead, I use &lt;a href="https://github.com/drone/drone"&gt;Drone&lt;/a&gt; a container-first Continuous Delivery platform built on Docker, and written in Go. It's still a fairly young project, but what's there currently is more than sufficient for my needs. &lt;/p&gt;
&lt;p&gt;Drone integrates with Github and Rancher! All I have to do is add a configuration file called &lt;code&gt;.drone.yml&lt;/code&gt; to my root directory and place it with git. It makes my life easy by automatically connecting to my Github account and configuring selected repositories for webhook delivery. When I push code to one of my repos, Drone is automatically notified and a build begins automatically. &lt;/p&gt;
&lt;p&gt;A build consists of first, testing the code using application-specific tests in each repository. Then if all that passes, a Docker container with the application is packaged up and uploaded to the Dockerhub image registry. Lastly, once the image has arrived at the registry, Drone makes an API call to Rancher and upgrades an existing service in place with the new image. &lt;/p&gt;
&lt;p&gt;Regardless of success or failure, Drone then sends a Slack message informing me of the outcome and a link to the build UI. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Drone CI Screenshot" src="https://connerswann.me/images/2017/drone-ci-screenshot.png"&gt;&lt;/p&gt;
&lt;h1&gt;Putting it Together&lt;/h1&gt;
&lt;p&gt;To demonstrate the power of this CI system, I made a quick little example project that I've thrown up on Github. It's a little flask app that returns "Hello World!" when you make a GET request to it. &lt;/p&gt;
&lt;p&gt;It's hosted here. But here's some of the juicy code so you don't have to go so far. &lt;/p&gt;
&lt;p&gt;This is the &lt;code&gt;.drone.yml&lt;/code&gt; file, which is basically where all the magic happens:&lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-python"&gt;
pipeline:
  test:
    image: python:2.7
    commands: 
      - pip install -r application/requirements.txt
      - python application/tests.py
  publish:
    image: docker
    commands:
      - docker login -u "$DOCKER_USERNAME" -p "$DOCKER_PASSWORD"
      - docker build -t yourbuddyconner/docker-hello-world .
      - docker push yourbuddyconner/docker-hello-world
    environment:
      - DOCKER_USERNAME=${DOCKER_USERNAME}
      - DOCKER_PASSWORD=${DOCKER_PASSWORD}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
  deploy: 
    image: peloton/drone-rancher
    url: http://rancher.swannairlines.com
    access_key: ${RANCHER_ACCESS}
    secret_key: ${RANCHER_SECRET}
    service: hello-world/hello-world
    docker_image: yourbuddyconner/docker-hello-world:latest
    confirm: true
    timeout: 120
  slack:
    image: plugins/slack
    webhook: https://hooks.slack.com/services/...
    recipient: conner
    template: &gt;
      {{#success build.status}}
        Build {{build.number}}  for {{repo.name}}/{{build.branch}} succeeded. Good job.
      {{else}}
        Build {{build.number}}  for {{repo.name}}/{{build.branch}} Failed. You suck.
      {{/success}}
      Build Link: {{build.link}}
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;This setup makes it ridiculously easy for me to launch a large number of containers running homebrew software with minimal interference from me. I can push code, go make coffee, and come back knowing that there's a new build of my code online. Well... either that or I messed up somewhere and I'm going to have a lead on how to fix it when I sit down.&lt;/p&gt;
&lt;p&gt;If you have any questions or comments about this blog post, feel free to tweet me &lt;a href="http://twitter.com/yourbuddyconner"&gt;@Yourbuddyconner&lt;/a&gt;!&lt;/p&gt;</content><category term="Infrastructure"></category><category term="continuous integration"></category><category term="rancher"></category><category term="docker"></category><category term="drone-ci"></category></entry><entry><title>You've Got Junk In Your Splunk (Part 2) - An Examination of Real-World Use-Cases for Splunk</title><link href="https://connerswann.me/2015/11/youve-got-junk-in-your-splunk-part-2.html" rel="alternate"></link><published>2015-11-30T00:00:00-08:00</published><updated>2015-11-30T00:00:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-11-30:/2015/11/youve-got-junk-in-your-splunk-part-2.html</id><summary type="html">&lt;p&gt;&lt;small&gt;This is Part 2 of a multi-part post about the amazing software that is Splunk, if you haven't already, head over to &lt;a href="http://connerswann.me/youve-got-junk-in-your-splunk-an-introduction-to-splunk-and-it-data-analysis/"&gt;Part 1&lt;/a&gt; and check it out.&lt;/small&gt;&lt;/p&gt;
&lt;h2&gt;Case Studies&lt;/h2&gt;
&lt;p&gt;There is a virtually limitless set of problems that Splunk easily solves. From the usual IT systems analysis to advanced …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;small&gt;This is Part 2 of a multi-part post about the amazing software that is Splunk, if you haven't already, head over to &lt;a href="http://connerswann.me/youve-got-junk-in-your-splunk-an-introduction-to-splunk-and-it-data-analysis/"&gt;Part 1&lt;/a&gt; and check it out.&lt;/small&gt;&lt;/p&gt;
&lt;h2&gt;Case Studies&lt;/h2&gt;
&lt;p&gt;There is a virtually limitless set of problems that Splunk easily solves. From the usual IT systems analysis to advanced visualization of business trends and statistics, you'd be hard-pressed to find a decently-sized analytics task that Splunk would preform poorly with. &lt;/p&gt;
&lt;p&gt;To illustrate the vast array of &lt;em&gt;things&lt;/em&gt; Splunk can do, in this and subsequent posts, I'm going to examine two different use-cases for Splunk. This one looks at it from a Business-Analytics perspective and the next one will be from a more traditional IT Intrusion Prevention perspective. &lt;/p&gt;
&lt;h1&gt;7/11 - Indonesia&lt;/h1&gt;
&lt;p&gt;&lt;img alt="7/11 Logo" src="https://connerswann.me/images/2015/7-Eleven-Logo.PNG"&gt;&lt;/p&gt;
&lt;h2&gt;Pre-Splunk&lt;/h2&gt;
&lt;p&gt;In 2009 7/11, the US gas station and convenience store chain, decided to expand to Indonesia, a new and foreign market to them. This obviously required them to come up with new promotions, sell new products, and predict when customers were going to want a particular product so they were sure to have it in stock.&lt;/p&gt;
&lt;p&gt;To be successful in the new region, the company had to compete with pre-existing &lt;a href="https://en.wikipedia.org/wiki/Warung"&gt;"warungs"&lt;/a&gt; -- small casual cafes and shops that are ubiquitous across the island nation. To do this, 7/11 stores in the country sold local foods along with the usual soft drinks and convenience store items. In addition, stores also provide amenities like outdoor seating, WiFi, and music to customers to make them more hospitable to a younger middle-class clientele. This allowed them to not only establish a foothold in the country, but thrive and expand to 100+ stores nationwide.&lt;/p&gt;
&lt;p&gt;In order to maintain a healthy revenue stream and stay on top of their competition, 7/11 set up information-gathering infrastructure in order to identify trends in their Point of Sale data. However, this system was designed and built on top of their legacy IT systems and was markedly rigid in its processes. As a result, while they were able to glean insights from their data, the process required actual people to manually analyze data for significant portions of the process. From the moment they decided to process data in a time range, it could take as long as 3 months to organize and execute a marketing campaign. While the data &lt;em&gt;was&lt;/em&gt; available to them and provided useful insights on what their customers wanted at a particular time, the process was far from real-time analysis and ultimately could be improved. &lt;/p&gt;
&lt;h2&gt;Post-Splunk&lt;/h2&gt;
&lt;p&gt;In 2014, the company began searching for another data analysis platform to give them an edge in Indonesia, and after vetting several other competitors in the space, ended up choosing Splunk. By forwarding logs from Point of Sale systems in each location to a central Splunk cluster and taking advantage of the real-time nature of Splunk's data analytics, 7/11 was able to drastically increase the efficiency of its marketing and promotions machine. &lt;/p&gt;
&lt;p&gt;By adopting Splunk, the company cut their time-to-promotions by over 80%. Whereas before it took over three months to plan and prepare a marketing campaign, after Splunk it only takes them two week's time from the moment they decide to put together a campaign to the point where the campaign is live. In addition, Splunk allows 7/11 to take other data sources into account when deciding what products to stock at any particular time. By consuming weather forecasts from the &lt;a href="https://developer.yahoo.com/weather/"&gt;Yahoo! Weather API&lt;/a&gt;, they are able to predict when certain products will be in high demand days or weeks in advance. &lt;/p&gt;
&lt;p&gt;As I hope you can see, Splunk is immensely useful when collecting data from IT resources to affect business decisions. The next post in this series will examine how I personally used Splunk at work for a proof-of-concept in Information Security. I leveraged Splunk by collecting data from a series of Raspberry Pi SSH HoneyPots and creating Splunk Alerts when potentially malicious activity appears elsewhere in our system. Stay Tuned!&lt;/p&gt;
&lt;p&gt;Source: 
&lt;a href="http://www.splunk.com/view/7-eleven-indonesia/SP-CAAAN92"&gt;Splunk Success Story - 7/11&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Do you use Splunk? Have some good ideas about creating value through aggregate data? Hit me up on Twitter and let's talk! &lt;a href="http://twitter.com/yourbuddyconner"&gt;@YourBuddyConner&lt;/a&gt;&lt;/p&gt;</content><category term="Splunk"></category><category term="Splunk"></category><category term="Case Study"></category></entry><entry><title>You've Got Junk In Your Splunk (Part 1) - An Introduction to Splunk and IT Data Analysis</title><link href="https://connerswann.me/2015/11/youve-got-junk-in-your-splunk-part-1.html" rel="alternate"></link><published>2015-11-20T22:17:00-08:00</published><updated>2015-11-20T22:17:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-11-20:/2015/11/youve-got-junk-in-your-splunk-part-1.html</id><summary type="html">&lt;p&gt;If you've read some of my older posts, you might have seen &lt;a href="http://connerswann.me/splunk-analyzing-text-messages/"&gt;this one&lt;/a&gt; about my personal analytics project project where I used Splunk to index 50,000+ text messages and preform analysis on them. &lt;/p&gt;
&lt;p&gt;Keeping with the Splunk theme, early this fall I put together a classroom presentation on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you've read some of my older posts, you might have seen &lt;a href="http://connerswann.me/splunk-analyzing-text-messages/"&gt;this one&lt;/a&gt; about my personal analytics project project where I used Splunk to index 50,000+ text messages and preform analysis on them. &lt;/p&gt;
&lt;p&gt;Keeping with the Splunk theme, early this fall I put together a classroom presentation on the Splunk to introduce students at Northern Arizona University to the software and help them understand the sorts of use-cases it is really useful for. Today, I decided to take the general information contained within the presentation and turn it into a handy blog post for those people who might be googling "What the heck is Splunk?!" or something similar. &lt;/p&gt;
&lt;p&gt;This is part one in a multi-part post. Today I'll discuss the multitude of problems Splunk is targeting and how it uniquely solves them in a way that is capable of affecting how an entire business handles their day-to-day workflow. ~~Stay tuned for~~ &lt;a href="http://connerswann.me/youve-got-junk-in-your-splunk-part-2-an-examination-of-real-world-use-cases-for-splunk/"&gt;Here's the next post&lt;/a&gt; where I'll put the Splunk workflow into context with real-world use cases!&lt;/p&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;In this day and age, the majority of "Enterprise Data" is generated by machines preforming a variety of different functions at any given time. Often-times, this data that's generated and saved to log files is not human-readable, and when it &lt;em&gt;is&lt;/em&gt; human-readable there's often too much of it to process manually. &lt;/p&gt;
&lt;p&gt;This data comes from tens, hundreds, or even thousands of different applications, all of whom potentially output their logs in as many different formats as there are applications. &lt;/p&gt;
&lt;p&gt;Splunk has a multitude of use-cases, from IT Analytics to planning marketing campaigns. The following are some problems people in varying positions of might use Splunk to solve:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dan the developer is asked to help figure out why his code is crashing on Sundays at Midnight&lt;/li&gt;
&lt;li&gt;Sally the SysAdmin has no idea why users from one office location can’t log in to their computers&lt;/li&gt;
&lt;li&gt;Ivan the InfoSec Analyst has no idea a hacker in Bulgaria is sending spam from his servers&lt;/li&gt;
&lt;li&gt;Billy the Business Analyst needs to figure out what localities are using his company’s applications&lt;/li&gt;
&lt;li&gt;Molly the Marketing Executive needs to analyze her affiliate marketing campaigns to see if improvements can be made&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Machine Data is the most rapidly growing segment of the arena experts call "&lt;a href="https://en.wikipedia.org/wiki/Big_data"&gt;Big Data&lt;/a&gt;." This stuff gets generated 24 hours a day, 7 days a week, 365 days a year and will continue to be generated forever. This set of data contains a categorical record of everything a user does online (and oftentimes offline). The main problem is that the value from this data is, for the most part, untapped -- the data sits on a server until it gets deleted to make room for more data that rarely (or never) gets looked at. &lt;/p&gt;
&lt;h2&gt;What Does Machine Data Look Like?&lt;/h2&gt;
&lt;p&gt;When talking about how difficult "Machine Data" is to process manually, it's really important to be able to visualize exactly why it's so difficult to process. Below are some examples text that Splunk might consume: &lt;/p&gt;
&lt;h4&gt;HoneyPot Logs:&lt;/h4&gt;
&lt;pre class="language-accesslog"&gt;
&lt;code class="language-accesslog" data-language="Honeypot Log"&gt;
2015-10-17 13:08:51-0700 [SSHService ssh-userauth on HoneyPotTransport,2323,93.158.203.167] login attempt [root/12345] succeeded
&lt;/code&gt;
&lt;/pre&gt;

&lt;h4&gt;Webserver Logs:&lt;/h4&gt;
&lt;pre class="language-accesslog"&gt;
&lt;code class="language-accesslog" data-language="Apache Access Log"&gt;
64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] "GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1" 401 12846
&lt;/code&gt;
&lt;/pre&gt;

&lt;h4&gt;Tweets:&lt;/h4&gt;
&lt;pre class="line-numbers language-json"&gt;
&lt;code class="language-json" data-language="json"&gt;
{
    "created_at":"Mon Sep 28 19:39:04 +0000 2015”, 
    ”user”:”yourbuddyconner", 
    "id":648582717068587000, 
    "id_str":"648582717068587009", 
    "text":"The amount of local news stations treating the Facebook outage as news is too damn high. #FacebookDown #TwitterIsUp #Facebook”, 
    "entities": {
        "hashtags":[
            {
                "text":"FacebookDown",
                "indices":[89,102]
            }, 
            {
                "text":"TwitterIsUp", 
                "indices":[103,115]
            }, 
            {
                "text":"Facebook", 
                "indices":[116,125]
            }
        ], 
        "symbols":[], 
        "user_mentions":[], 
        "urls":[]
    }
}
&lt;/code&gt;
&lt;/pre&gt;

&lt;h4&gt;Text Messages:&lt;/h4&gt;
&lt;pre class="language-accesslog"&gt;
&lt;code class="language-accesslog" data-language="text-message"&gt;
message_id=53088 
timestamp="2015-02-03 20:30:06" 
date_read="2015-02-03 20:29:20" 
is_from_me=1 
is_read=1 
handle=+9999999999 
service=iMessage 
message="I mean, I can, those pancakes were so good"
&lt;/code&gt; 
&lt;/pre&gt;

&lt;p&gt;As I hope you can see, the data above is incredibly dense and hard to parse by the human eye unless you're trained to know what to look for. Even to the untrained eye though, if you look closely, there's a &lt;em&gt;lot&lt;/em&gt; of data contained in these short excerpts -- there's timestamps, hashtags, messages, HTTP Methods, IDs and more!&lt;/p&gt;
&lt;p&gt;When you think about it, it might be possible to connect these discrete events together, timestamp to timestamp, username to username, etc. If only you had a platform that could read and understand these disparate formats!&lt;/p&gt;
&lt;h2&gt;Enter Splunk&lt;/h2&gt;
&lt;p&gt;Well, lucky for you, Splunk does just that! Splunk fills several roles when it comes to &lt;a href="https://en.wikipedia.org/wiki/Operational_intelligence"&gt;"Operational Intelligence"&lt;/a&gt;. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href="http://docs.splunk.com/Splexicon:Universalforwarder"&gt;Splunk Universal Forwarder&lt;/a&gt; collects lines from log files as they are added to end-systems and forwards them to the Splunk Indexer.&lt;/li&gt;
&lt;li&gt;The &lt;a href="http://docs.splunk.com/Splexicon:Indexer"&gt;Splunk Indexer&lt;/a&gt; parses log events sent to it from the Universal Forwarder or by other means, splits them into key-value fields, and stores them for safe-keeping&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, this may not seem  too groundbreaking, however, when you consider that this interaction occurs constantly and that old data gets saved in Splunk's data warehouse, there's a lot of power to be had. &lt;/p&gt;
&lt;h3&gt;Business Reactivity&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Business Reactivity Graph" src="https://connerswann.me/images/2015/business-reactivity.png"&gt;&lt;/p&gt;
&lt;p&gt;The diagram above describes different stages of "Reactivity" in a business. At the lower left we have, "Search and Investigate." A business at this stage finds out things are broken hours or sometimes days after-the fact. Fixing a problem a business like this might be facing often requires a person to manually read over days of log files to figure out what went wrong. &lt;/p&gt;
&lt;p&gt;The two intermediate states "Proactive Monitoring and Alerting" and "Operational Visibility" describe a business who has some good practices in place that allow them to be less reactive. Operational Visibility is the ability to &lt;em&gt;see&lt;/em&gt; how each part of your system is communicating with one another, usually in one place, a dashboard for example. (As an aside, Netflix has a really good &lt;a href="http://techblog.netflix.com/2014/01/improving-netflixs-operational.html"&gt;Blog Post&lt;/a&gt; about how they use real-time data analysis to improve their Operational Visibility.) This entails knowing when and what sorts of data is being passed around at any discrete point in time. This is immensely useful when a piece of a distributed system is acting up and someone needs to get down to fixing it. &lt;/p&gt;
&lt;p&gt;The upper-right state is "Real-Time Business Insights." A business who has processes in place to achieve this is in the best shape of all when it comes to being proactive. It implies that the business has both historical and real-time data to analyze and make up-to-the-minute decisions with. Splunk takes businesses immediately to this stage if well thought-out and implemented.&lt;/p&gt;
&lt;h3&gt;Splunk Helps You to be "Proactive"&lt;/h3&gt;
&lt;p&gt;By storing historical data, Splunk allows a user to determine baselines in activity and look at how those baselines have changed over time. With a little help from someone who knows the Splunk query language, it can identify what is considered to be "normal" behavior and also anomalous events that might affect business decisions. &lt;/p&gt;
&lt;p&gt;In addition, Splunk enables the IT Professional to share their complex data with people who might not be as versed in it as they are. This is done through the use of visualizations and graphs which can then be inserted into &lt;a href="http://docs.splunk.com/Splexicon:Dashboard"&gt;Splunk Dashboards&lt;/a&gt; for repeated use. This removed the overhead of having to know what the data looks like, and instead digesting insights in aggregate through pictures. &lt;/p&gt;
&lt;p&gt;By implementing a Splunk Cluster on-site, many successful and well-known companies have leveraged the power of Splunk to increase the value of the company as a whole and provide value to pre-existing business decision-making processes.&lt;/p&gt;
&lt;p&gt;In my &lt;a href="http://connerswann.me/youve-got-junk-in-your-splunk-part-2-an-examination-of-real-world-use-cases-for-splunk/"&gt;next post&lt;/a&gt;, I'll be going over some real use-cases for Splunk and case-studies that examine Splunk use in the wild. Stay tuned! &lt;/p&gt;
&lt;p&gt;Do you use Splunk? Hit me up on Twitter and let's talk! &lt;a href="http://twitter.com/yourbuddyconner"&gt;@yourbuddyconner&lt;/a&gt;&lt;/p&gt;</content><category term="Splunk"></category><category term="Splunk"></category><category term="Case Study"></category></entry><entry><title>Drones: My Foray Into Aerial Videography and Photography</title><link href="https://connerswann.me/2015/06/drones-my-foray-into-aerial-videography-and-photography.html" rel="alternate"></link><published>2015-06-27T20:49:00-07:00</published><updated>2015-06-27T20:49:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-06-27:/2015/06/drones-my-foray-into-aerial-videography-and-photography.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="https://connerswann.me/images/2015/drones-cover-pano.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Several months ago, I purchased a DJI Phantom 2 Vision from a friend. Since then I've logged about 40 hours flying it, and at this point I can honestly say it's one of the best purchases I've ever made. The Phantom's size and manuverability make it the perfect solution to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="https://connerswann.me/images/2015/drones-cover-pano.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Several months ago, I purchased a DJI Phantom 2 Vision from a friend. Since then I've logged about 40 hours flying it, and at this point I can honestly say it's one of the best purchases I've ever made. The Phantom's size and manuverability make it the perfect solution to capture dozens of shots that had previously been rendered impossible without lots of expensive equipment and/or a helicopter rental. &lt;/p&gt;
&lt;p&gt;I've been having loads of fun taking it with me and finding opportunities to get a cool shot. &lt;/p&gt;
&lt;blockquote class="instagram-media" data-instgrm-version="4" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 0 auto; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);"&gt;&lt;div style="padding:8px;"&gt; &lt;div style=" background:#F8F8F8; line-height:0; margin-top:40px; padding:50% 0; text-align:center; width:100%;"&gt; &lt;div style=" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAAGFBMVEUiIiI9PT0eHh4gIB4hIBkcHBwcHBwcHBydr+JQAAAACHRSTlMABA4YHyQsM5jtaMwAAADfSURBVDjL7ZVBEgMhCAQBAf//42xcNbpAqakcM0ftUmFAAIBE81IqBJdS3lS6zs3bIpB9WED3YYXFPmHRfT8sgyrCP1x8uEUxLMzNWElFOYCV6mHWWwMzdPEKHlhLw7NWJqkHc4uIZphavDzA2JPzUDsBZziNae2S6owH8xPmX8G7zzgKEOPUoYHvGz1TBCxMkd3kwNVbU0gKHkx+iZILf77IofhrY1nYFnB/lQPb79drWOyJVa/DAvg9B/rLB4cC+Nqgdz/TvBbBnr6GBReqn/nRmDgaQEej7WhonozjF+Y2I/fZou/qAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;"&gt;&lt;/div&gt;&lt;/div&gt;&lt;p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;"&gt;&lt;a href="https://instagram.com/p/36vvsHHDH9/" style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none;" target="_top"&gt;A video posted by Conner Swann (@swannairlines)&lt;/a&gt; on &lt;time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2015-06-14T17:26:51+00:00"&gt;Jun 14, 2015 at 10:26am PDT&lt;/time&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;script async defer src="//platform.instagram.com/en_US/embeds.js"&gt;&lt;/script&gt;
&lt;blockquote&gt;
&lt;p&gt;St. Marys College, Moraga, California&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="instagram-media" data-instgrm-version="4" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 0 auto; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);"&gt;&lt;div style="padding:8px;"&gt; &lt;div style=" background:#F8F8F8; line-height:0; margin-top:40px; padding:50% 0; text-align:center; width:100%;"&gt; &lt;div style=" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAAGFBMVEUiIiI9PT0eHh4gIB4hIBkcHBwcHBwcHBydr+JQAAAACHRSTlMABA4YHyQsM5jtaMwAAADfSURBVDjL7ZVBEgMhCAQBAf//42xcNbpAqakcM0ftUmFAAIBE81IqBJdS3lS6zs3bIpB9WED3YYXFPmHRfT8sgyrCP1x8uEUxLMzNWElFOYCV6mHWWwMzdPEKHlhLw7NWJqkHc4uIZphavDzA2JPzUDsBZziNae2S6owH8xPmX8G7zzgKEOPUoYHvGz1TBCxMkd3kwNVbU0gKHkx+iZILf77IofhrY1nYFnB/lQPb79drWOyJVa/DAvg9B/rLB4cC+Nqgdz/TvBbBnr6GBReqn/nRmDgaQEej7WhonozjF+Y2I/fZou/qAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;"&gt;&lt;/div&gt;&lt;/div&gt;&lt;p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;"&gt;&lt;a href="https://instagram.com/p/3HZkS0H17k/" style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none;" target="_top"&gt;A video posted by Conner Swann (@yourbuddyconner)&lt;/a&gt; on &lt;time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2015-05-25T18:51:47+00:00"&gt;May 25, 2015 at 11:51am PDT&lt;/time&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;script async defer src="//platform.instagram.com/en_US/embeds.js"&gt;&lt;/script&gt;
&lt;blockquote&gt;
&lt;p&gt;Diablo Shadows Park, Walnut Creek&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="instagram-media" data-instgrm-version="4" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 0 auto; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);"&gt;&lt;div style="padding:8px;"&gt; &lt;div style=" background:#F8F8F8; line-height:0; margin-top:40px; padding:50% 0; text-align:center; width:100%;"&gt; &lt;div style=" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAAGFBMVEUiIiI9PT0eHh4gIB4hIBkcHBwcHBwcHBydr+JQAAAACHRSTlMABA4YHyQsM5jtaMwAAADfSURBVDjL7ZVBEgMhCAQBAf//42xcNbpAqakcM0ftUmFAAIBE81IqBJdS3lS6zs3bIpB9WED3YYXFPmHRfT8sgyrCP1x8uEUxLMzNWElFOYCV6mHWWwMzdPEKHlhLw7NWJqkHc4uIZphavDzA2JPzUDsBZziNae2S6owH8xPmX8G7zzgKEOPUoYHvGz1TBCxMkd3kwNVbU0gKHkx+iZILf77IofhrY1nYFnB/lQPb79drWOyJVa/DAvg9B/rLB4cC+Nqgdz/TvBbBnr6GBReqn/nRmDgaQEej7WhonozjF+Y2I/fZou/qAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;"&gt;&lt;/div&gt;&lt;/div&gt;&lt;p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;"&gt;&lt;a href="https://instagram.com/p/3Fbzb7nDGP/" style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none;" target="_top"&gt;A video posted by Conner Swann (@swannairlines)&lt;/a&gt; on &lt;time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2015-05-25T00:32:51+00:00"&gt;May 24, 2015 at 5:32pm PDT&lt;/time&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;script async defer src="//platform.instagram.com/en_US/embeds.js"&gt;&lt;/script&gt;
&lt;blockquote&gt;
&lt;p&gt;Fish Ranch Road, California&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="instagram-media" data-instgrm-version="4" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 0 auto; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);"&gt;&lt;div style="padding:8px;"&gt; &lt;div style=" background:#F8F8F8; line-height:0; margin-top:40px; padding:50% 0; text-align:center; width:100%;"&gt; &lt;div style=" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAAGFBMVEUiIiI9PT0eHh4gIB4hIBkcHBwcHBwcHBydr+JQAAAACHRSTlMABA4YHyQsM5jtaMwAAADfSURBVDjL7ZVBEgMhCAQBAf//42xcNbpAqakcM0ftUmFAAIBE81IqBJdS3lS6zs3bIpB9WED3YYXFPmHRfT8sgyrCP1x8uEUxLMzNWElFOYCV6mHWWwMzdPEKHlhLw7NWJqkHc4uIZphavDzA2JPzUDsBZziNae2S6owH8xPmX8G7zzgKEOPUoYHvGz1TBCxMkd3kwNVbU0gKHkx+iZILf77IofhrY1nYFnB/lQPb79drWOyJVa/DAvg9B/rLB4cC+Nqgdz/TvBbBnr6GBReqn/nRmDgaQEej7WhonozjF+Y2I/fZou/qAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;"&gt;&lt;/div&gt;&lt;/div&gt;&lt;p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;"&gt;&lt;a href="https://instagram.com/p/1RYULoH10L/" style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none;" target="_top"&gt;A photo posted by Conner Swann (@yourbuddyconner)&lt;/a&gt; on &lt;time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2015-04-09T22:50:28+00:00"&gt;Apr 9, 2015 at 3:50pm PDT&lt;/time&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;script async defer src="//platform.instagram.com/en_US/embeds.js"&gt;&lt;/script&gt;
&lt;blockquote&gt;
&lt;p&gt;Lake Havasu, Arizona&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In addition to simply capturing single-frame photos, once the quadcopter is in the air, it's trivial to capture multiple stills and stitch them together with the magic of Photoshop. The results never fail to impress:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://connerswann.me/images/2015/drones-pano-1.jpg"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lake Havasu, Arizona&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="" src="https://connerswann.me/images/2015/drones-pano-2.jpg"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NAU Sky Dome, Flagstaff, Arizona&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="" src="https://connerswann.me/images/2015/drones-pano-3.jpg"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Buffalo Park, Flagstaff, Arizona &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All in all, I'm incredibly happy I got my hands on this wonderful piece of equipment. I've gotten way more than my money's worth, it was worth every penny! &lt;/p&gt;
&lt;p&gt;If you'd like to see more pictures or video, or keep track of my aerial adventures, &lt;a href="http://instagram.com/swannairlines"&gt;Follow me on Instagram!&lt;/a&gt;&lt;/p&gt;</content><category term="Video"></category><category term="Videography"></category><category term="Video"></category><category term="Photography"></category><category term="Drone"></category><category term="DJI"></category></entry><entry><title>Keeping Your University Public</title><link href="https://connerswann.me/2015/04/keeping-your-university-public.html" rel="alternate"></link><published>2015-04-20T22:09:00-07:00</published><updated>2015-04-20T22:09:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-04-20:/2015/04/keeping-your-university-public.html</id><summary type="html">&lt;p&gt;Over the past year or so, I've slowly become aware of the sheer amount of &lt;em&gt;stuff&lt;/em&gt; my school (Northern Arizona University) buys and how sometimes, I don't agree that a particular thing needed to be bought. Regardless of what's being purchased, from more residence halls and parking (I wish) to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Over the past year or so, I've slowly become aware of the sheer amount of &lt;em&gt;stuff&lt;/em&gt; my school (Northern Arizona University) buys and how sometimes, I don't agree that a particular thing needed to be bought. Regardless of what's being purchased, from more residence halls and parking (I wish) to contractors, it turns out that pretty much everything is published and freely available due to NAU being a public institution. &lt;/p&gt;
&lt;p&gt;A couple of days ago, one of my friends showed me &lt;a href="http://nau.edu/Contracting-Purchasing-Services/_Forms/Bids/P15JO001/"&gt;a Request for Proposals&lt;/a&gt; that NAU had published, detailing a intent to hire "Federal Lobbying Services and Public Relations Consulting." Now, that got me asking some questions. Why exactly does NAU need federal lobbyists and why is this the first time I was hearing about this? &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://connerswann.me/images/2015/keeping-university-public-lobbying-proposal.png"&gt;&lt;/p&gt;
&lt;p&gt;As a student, I definitely feel like I am far removed from decisions that often affect me and the general campus community directly. Questions like who is building the new science lab building or how much parking is needed seem to be answered without even consulting the people (students) that the university is (hypothetically) serving. While it is obvious that these proposals are being posted publically, they're posted in areas of the nau.edu website that might be "off the beaten path" for your average student. &lt;/p&gt;
&lt;p&gt;Now, I can agree that most of these decisions are boring and it makes sense to compartmentalize the decision-making process as much as possible. Furthermore, a lot of these issues are horrendously complex, so sometimes they're understandably difficult to summarize. However, I believe an attempt needs to be made to engage the student body, if only passively so that they know projects are taking place.&lt;/p&gt;
&lt;p&gt;It was with that sentiment that I decided to throw together a quick proof-of-concept project called &lt;a href="http://twitter.com/KeepNAUPublic"&gt;@KeepNAUPublic&lt;/a&gt;, a Twitter bot that watches the &lt;a href="http://nau.edu/Contracting-Purchasing-Services/NAU-Bid-Board/"&gt;NAU Bid Board&lt;/a&gt; and tweets the title, due date, and a link to the proposal whenever a new RFP is posted. &lt;/p&gt;
&lt;blockquote class="twitter-tweet tw-align-center" lang="en"&gt;&lt;p&gt;P15JO001&amp;#10;Req: Federal Lobbying Services and Public Relations Consulting&amp;#10;Link: &lt;a href="http://t.co/E9hoxfuxMO"&gt;http://t.co/E9hoxfuxMO&lt;/a&gt;&amp;#10;Due: April 21, 2015, at 2:00 PM&lt;/p&gt;&amp;mdash; Keep NAU Public (@KeepNAUPublic) &lt;a href="https://twitter.com/KeepNAUPublic/status/590240493943959552"&gt;April 20, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;Currently, the process is pretty basic, however as time goes on I forsee the ability to search proposals for keywords, red-flags, and other things that a student might care about. I forsee this being a nice visualization of when, where and how NAU makes purchasing decisions. &lt;/p&gt;
&lt;p&gt;The bot is implemented with a couple Python libraries including &lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;BeautifulSoup&lt;/a&gt; for HTML scraping and parsing, and &lt;a href="http://www.tweepy.org/"&gt;Tweepy&lt;/a&gt; which I used as a Twitter API Client. I've decided to hold off on posting the source code for now until the bot has been significantly more fleshed out, but if you have any questions about how I went about it, feel free to shoot me a tweet &lt;a href="http://twitter.com/yourbuddyconner"&gt;@YourBuddyConner&lt;/a&gt;!&lt;/p&gt;</content><category term="College"></category><category term="Freedom"></category><category term="NAU"></category><category term="Public Data"></category></entry><entry><title>Hack AZ: Postgame Report</title><link href="https://connerswann.me/2015/03/hack-az-postgame-report.html" rel="alternate"></link><published>2015-03-11T06:54:00-07:00</published><updated>2015-03-11T06:54:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-03-11:/2015/03/hack-az-postgame-report.html</id><summary type="html">&lt;p&gt;&lt;img alt="Hack AZ Cover" src="https://connerswann.me/2015/03/hack-az-cover.jpg"&gt;&lt;/p&gt;
&lt;p&gt;This past weekend, I went to &lt;a href="hackarizona.org"&gt;Hack Arizona&lt;/a&gt;, Arizona's first-ever college Hackathon hosted at the University of Arizona in Tucson. &lt;/p&gt;
&lt;h4&gt;hack·a·thon&lt;/h4&gt;
&lt;p&gt;ˈhakəˌTHän/
noun informal
&lt;em&gt;an event, typically lasting several days, in which a large number of people meet to engage in collaborative computer programming.
"a series of 48-hour …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Hack AZ Cover" src="https://connerswann.me/2015/03/hack-az-cover.jpg"&gt;&lt;/p&gt;
&lt;p&gt;This past weekend, I went to &lt;a href="hackarizona.org"&gt;Hack Arizona&lt;/a&gt;, Arizona's first-ever college Hackathon hosted at the University of Arizona in Tucson. &lt;/p&gt;
&lt;h4&gt;hack·a·thon&lt;/h4&gt;
&lt;p&gt;ˈhakəˌTHän/
noun informal
&lt;em&gt;an event, typically lasting several days, in which a large number of people meet to engage in collaborative computer programming.
"a series of 48-hour hackathons to build new web and mobile services"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Of all the trends the past couple of years have brought, I have to say that Hackathons are probably the coolest. Together in small groups (~1-6), college and high school-aged students get together and over the course of a sleep-deprived weekend, they create cool hardware or software projects! &lt;/p&gt;
&lt;p&gt;&lt;img alt="Hack AZ 1" src="https://connerswann.me/2015/03/hack-az-1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;I had found out a couple days before the event that Raytheon Missile Systems was sponsoring a drone challenge, and when I told my team the "Hack Jacks" (consisting of my friends Brandon Paree and Dylan Grayson, along with myself) about it, they were more than interested. &lt;/p&gt;
&lt;p&gt;Raytheon had purchased 15 &lt;a href="http://ardrone2.parrot.com/"&gt;Parrot AR.Drone 2.0&lt;/a&gt;'s, for the purpose of renting them out to teams at Hack AZ so they could write code and hack on them. Each member of the teams with the best hacks would then walk away with their own AR Drone! As luck would have it, we ended up getting to the UA campus early enough to snag one.&lt;/p&gt;
&lt;p&gt;The AR.Drone platform is an amazing beast, it's a quadcopter with a linux box its core. It also has a slick C software development kit, making it relatively easy to get these things to basically fly themselves. It turns out that a nice guy named &lt;a href="https://github.com/felixge/"&gt;felixge&lt;/a&gt; made a cool NodeJS library called &lt;a href="https://github.com/felixge/node-ar-drone"&gt;Node AR-Drone&lt;/a&gt; which ports many of those C bindings into the &lt;a href="https://www.youtube.com/watch?v=ame2PH67gnk"&gt;only real dev language&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Hack AZ Drone App" src="https://connerswann.me/2015/03/hack-az-drone-app.gif"&gt;&lt;/p&gt;
&lt;p&gt;We decided that we wanted to make our drone autonomous, so we leveraged the &lt;a href="http://opencv.org/"&gt;OpenCV&lt;/a&gt; Computer Vision library to do face tracking. &lt;/p&gt;
&lt;p&gt;In a nutshell, we built a basic AI that reads frames from the AR Drone's HD camera and searches them for faces. If no suitable objects are immediately visible, the drone enters a search pattern, slowly panning its view until it can see a face. Once it finds a trackable object, the drone changes course and follows it, adjusting its orientation and altitude to keep the face both centered and a constant size in its view (and thus, a constant distance away). It's key to note here that this incorporates basic guidance control elements as well as object identification and collision avoidance. &lt;/p&gt;
&lt;p&gt;This project was particularly challenging for the three of us, because none of us had any major graphics experience before. We ended up spending the majority of the time reading OpenCV source code and documentation. Unfortunately, with big projects like that it's easy to get lost in their complexity! However, despite the gaps in our knowledge, we picked a project that we felt we could demo well, and that we did. This is a major part of the hackathon, without a good demo, your project is nothing!&lt;/p&gt;
&lt;p&gt;When it came time to show off our hack, many of the other teams had trouble getting their drones off the ground, whether it be from quirky code written out of sleep deprevation or other hardware issues. Our presentation in contrast, went off incredibly smoothly. &lt;/p&gt;
&lt;p&gt;We showcased the facial recognition and search capabilities by first having my teammate Brandon stand in front of the drone to allow it to acquire a lock on his face. He then covered his face with a hand, waited as the drone panned over to the crowd, and much to our delight, it actually acquired a lock and drifted over to look at a spectator! I then had the entire group of people around me cover their faces, and agiain, the drone began to search for another target. &lt;/p&gt;
&lt;p&gt;After all the demos were done, the judges walked off the field to deliberate. After almost an hour, they came back with decisions and we ended up placing third out of more than ten teams!&lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture of Us" src="https://connerswann.me/2015/03/hack-az-2.jpg"&gt;&lt;/p&gt;
&lt;p&gt;This also meant we'd be going home with our very own Parrot AR Drones! &lt;/p&gt;
&lt;p&gt;Over the course of the weekend, I feel like I learned more than I really expected to. I met a bunch of really cool, like-minded people, and being in that sort of environment is both very enriching and a lot of fun. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The guys at Raytheon (Jonathan, Don, Dennis and more!) were a huge help to us, they went out to the field to help us test our program every time, giving us hours of their time. At the end of the weekend, the probably came out just as tired as we were!&lt;/li&gt;
&lt;li&gt;The Hack AZ organizers deserve a round of applause, we really appreciated the fact that the event had the entire Schence and Engineering Library at UA to itself for the majority of the weekend. &lt;/li&gt;
&lt;li&gt;Ellen, thanks for hanging out with us and taking all those cool pictures!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you'd like to check out our code, &lt;a href="https://github.com/yourbuddyconner/drone-hackaz/tree/master/Hack-AZ"&gt;the entire project is on Github&lt;/a&gt;. We've also begun to change and refactor our original gross hackathon code, and it's begun to take shape &lt;a href="https://github.com/yourbuddyconner/drone-hackaz/tree/master/v1.0"&gt;here&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Post-Hackathon, the project consists of an in-browser command-and-control center for the Parrot AR Drone, complete with gamepad support and first-person, bottom camera view, basic telemetry and a battery indicator. There are plenty of features on the roadmap, including more autonomous functionality and FPV goggle support. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Hack AZ Drone App Gif" src="https://connerswann.me/2015/03/hack-az-drone-app.gif"&gt;&lt;/p&gt;
&lt;p&gt;If you're interested in drones, interested in the project, or have any cool tips or tricks when it comes to Javascript image manipulation, feel free to give me a shoutout on Twitter &lt;a href="http://twitter.com/yourbuddyconner"&gt;@YourBuddyConner&lt;/a&gt;&lt;/p&gt;</content><category term="College"></category><category term="Hackathon"></category><category term="Hack Arizona"></category><category term="Drone"></category><category term="Raytheon"></category></entry><entry><title>Personal Analytics: Gleaning Metadata from Text Messages With Splunk</title><link href="https://connerswann.me/2015/02/personal-analytics-gleaning-metadata-from-text-messages-with-splunk.html" rel="alternate"></link><published>2015-02-06T05:58:00-08:00</published><updated>2015-02-06T05:58:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-02-06:/2015/02/personal-analytics-gleaning-metadata-from-text-messages-with-splunk.html</id><summary type="html">&lt;p&gt;&lt;img alt="Splunk Logo" src="https://connerswann.me/images/2015/personal-analytics-splunk-logo.gif"&gt;
&lt;/hr&gt;
I've been using &lt;a href="http://www.splunk.com/"&gt;Splunk&lt;/a&gt; a lot at work. According to their website: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You see servers and devices, apps and logs, traffic and clouds. We see data—everywhere. Splunk® offers the leading platform for Operational Intelligence. It enables the curious to look closely at what others ignore—machine data—and find …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Splunk Logo" src="https://connerswann.me/images/2015/personal-analytics-splunk-logo.gif"&gt;
&lt;/hr&gt;
I've been using &lt;a href="http://www.splunk.com/"&gt;Splunk&lt;/a&gt; a lot at work. According to their website: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You see servers and devices, apps and logs, traffic and clouds. We see data—everywhere. Splunk® offers the leading platform for Operational Intelligence. It enables the curious to look closely at what others ignore—machine data—and find what others never see: insights that can help make your company more productive, profitable, competitive and secure. What can Splunk do for you? Just ask.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My team uses the software to index and search through wide-ranging, potentially disparate datasources like server logs, google logins, data from our honeypot, and more. It fills so many use-cases, from tracking down users who have had their passwords phished to allowing us to pinpoint machines which are being tampered with (thanks to the wonder that is &lt;a href="http://www.ossec.net/"&gt;OSSEC&lt;/a&gt;). It's got a &lt;em&gt;really&lt;/em&gt; steep learning curve, but once you get the hang of it, administering a distributed Splunk environment is actually pretty fun. &lt;/p&gt;
&lt;p&gt;Since I've been using Splunk so much, I wanted to give it a spin on a personal project. I had read at some point that iOS's text messages are stored in a SQLite database, so I decided to try and pull that data out and do some analysis on it. &lt;/p&gt;
&lt;p&gt;SQLite has the advantage of being a flat file, making it perfectly suited for applications where network access either isn't needed or isn't readily available. Luckily for me, the &lt;a href="https://apps.splunk.com/app/958/"&gt;Splunk DB Connect&lt;/a&gt; app can natively parse SQLite and index the data as key-value pairs. &lt;/p&gt;
&lt;p&gt;As it turns out, the iOS text messaging database has been pretty heavily documented in the past, and by using the information &lt;a href="https://theiphonewiki.com/wiki/Messages#Indexes"&gt;here&lt;/a&gt; I was able to extract the text message database from a backup of my iPhone. There were a lot of entries in this 37MB database, the messages table has over 50,000 entries, each one representing either an outgoing or incoming text message. &lt;/p&gt;
&lt;p&gt;The database also has a 'Handles' table, which stores information about the phone number the user is communicating with. Without joining the two tables, the data is incomplete, so I  crafted a SQL query to grab the data I wanted to index from the table. This query is fed to the Splunk DB Connect app which then takes care of parsing out all 53,024 rows and splitting them up into indivudual events.  &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-sql"&gt;
SELECT message.ROWID as message_id,
    datetime(message.date,'unixepoch', '+31 years', '-6 hours') as timestamp, 
    datetime(message.date_read, 'unixepoch', '+31 years', '-6 hours') as date_read, 
    message.is_from_me as is_from_me,
    message.is_read as is_read,
    handle.id as handle, 
    message.service as service, 
    message.text as message FROM message, 
    handle 
WHERE message.handle_id = handle.ROWID;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;small&gt;The SQL Query to create complete events&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Once Splunk has digested the data, heading to the Search &amp;amp; Reporting app and typing &lt;code&gt;index=text_messages&lt;/code&gt; into the search box gives me a nice frequency graph and a list of events: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Splunk Screenshot" src="https://connerswann.me/images/2015/personal-analytics-screenshot-1.png"&gt;&lt;/p&gt;
&lt;p&gt;Each text message event is just a line of text that looks like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;message_id=53091 timestamp="2015-02-03 20:31:38" date_read="2015-02-03 20:31:28" is_from_me=1 is_read=1 handle=+xxxxxxxxxx service=iMessage message="I know, haha"&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;Using Splunk's powerful search tools and some apps created by the Splunk community, I could begin doing some batch processing of all the events. &lt;/p&gt;
&lt;p&gt;Since I have so much text, the first thing I did was download a &lt;a href="https://apps.splunk.com/app/1179/"&gt;Sentiment Analysis&lt;/a&gt; app. Doing so adds a couple dashboards and the "sentiment" search command which will evaluate one field in every event you give it, assigning the event a positive or negative value. &lt;/p&gt;
&lt;p&gt;Armed with this new field, I can now create some inferential data about the contents of the database as a whole. &lt;/p&gt;
&lt;p&gt;For example, I can chart average sentiment over time. To do this, you chain together multiple splunk search commands until the desired output is reached:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;index=text_messages  is_from_me=1 | sentiment twitter message | timechart avg(sentiment) as sentiment span=1mon&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In english, it says:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In index &lt;code&gt;text_messages&lt;/code&gt;, search for events which have &lt;code&gt;is_from_me&lt;/code&gt; equal to 1.&lt;/li&gt;
&lt;li&gt;After that, run a sentiment analysis on the results' &lt;code&gt;message&lt;/code&gt; field. &lt;/li&gt;
&lt;li&gt;Then use the results to chart the average of the &lt;code&gt;sentiment&lt;/code&gt; field over time spans of one month."&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The resulting graph looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="sentiment over time" src="https://connerswann.me/images/2015/personal-analytics-screenshot-2.png"&gt;&lt;/p&gt;
&lt;p&gt;By chaining together more complex series of commands, you can get some pretty cool results! &lt;/p&gt;
&lt;p&gt;The next graph is created by finding the average sentiment and forming a baseline sentiment value, allowing good and bad sentiment to cancel each other out, leaving only a remainder. &lt;/p&gt;
&lt;p&gt;Looking at the blue sentiment portion of the graph, zero is this baseline. Any column that falls below this has a "below average" sentiment while columns that are greater than zero posess an "above average" sentiment as compared with the rest of the events. &lt;/p&gt;
&lt;p&gt;The second yellow line simply charts the number of events in each period. &lt;/p&gt;
&lt;p&gt;&lt;code&gt;index=text_messages  is_from_me=1 | sentiment twitter message |eval diff=sentiment-0.788400| eval count=count|  timechart avg(diff) as sentiment, count span=14d&lt;/code&gt;
&lt;/br&gt;
&lt;img alt="Splunk Screenshot" src="https://connerswann.me/images/2015/personal-analytics-screenshot-3.png"&gt;&lt;/p&gt;
&lt;p&gt;This last one is my favorite. It overlays the sentiment of incoming messages over the sentiment of outgoing messages. This was to test the hypothesis that the sentiment of incoming texts could affect the sentiment of outgouing texts. While correlation doesn't imply causation, I still think it's interesting that the graphs follow each other in so many places! &lt;/p&gt;
&lt;p&gt;&lt;code&gt;index=text_messages  is_from_me=0 |  sentiment twitter message |  eval diff=sentiment-0.788400 | timechart avg(diff) as sentiment_from span=1mon | appendcols [search index=text_messages  is_from_me=1 |  sentiment twitter message |  eval diff2=sentiment-0.788400 | timechart avg(diff2) as sentiment_me span=1mon]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Splunk Screenshot" src="https://connerswann.me/images/2015/personal-analytics-screenshot-4.png"&gt;&lt;/p&gt;
&lt;p&gt;It's truly fascinating how each of these events are pretty useless by themselves, however when you take them all into account some interesting information can be gleaned. I plan to continue on with this little Personal Analytics project by incorporating more data sources like sleep analysis, stay tuned! &lt;/p&gt;
&lt;p&gt;If you're interested in this sort of thing, shoot me a tweet &lt;a href="http://twitter.com/yourbuddyconner"&gt;@YourBuddyConner&lt;/a&gt;, I'd love to hear what you think!&lt;/p&gt;</content><category term="Splunk"></category><category term="Splunk"></category><category term="Analytics"></category><category term="Sentiment Analysis"></category><category term="Text Message"></category><category term="iOS"></category></entry><entry><title>Ionic: Dealing with Remote Events on iOS</title><link href="https://connerswann.me/2015/01/ionic-dealing-with-remote-events-on-ios.html" rel="alternate"></link><published>2015-01-23T18:03:00-08:00</published><updated>2015-01-23T18:03:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-01-23:/2015/01/ionic-dealing-with-remote-events-on-ios.html</id><summary type="html">&lt;p&gt;For the past couple semesters, I've been working on an &lt;a href="http://ionicframework.com/"&gt;Ionic&lt;/a&gt; app for my university's radio station. Ionic is a "hybrid" framework, that is, the main body of the app is written using web technology (AngularJS, HTML, CSS) and that is coupled with Cordova, a piece of softeware that provides …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For the past couple semesters, I've been working on an &lt;a href="http://ionicframework.com/"&gt;Ionic&lt;/a&gt; app for my university's radio station. Ionic is a "hybrid" framework, that is, the main body of the app is written using web technology (AngularJS, HTML, CSS) and that is coupled with Cordova, a piece of softeware that provides an interface between the web view and the bare metal of the phone. &lt;/p&gt;
&lt;p&gt;Among other things, the app needed to be able to play the station's audio stream. Since I've been focusing primarily on the iOS portion of the universal app, a while back I decided it would be really cool if I were able to control the audio from the lockscreen. However, while the documentation for the Objective C side of it is pretty abundant, there's little to none discussing connecting this with Cordova. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Lockscreen Controls" src="https://connerswann.me/images/2015/ionic-ios-screenshot.png"&gt;&lt;/p&gt;
&lt;p&gt;Not knowing exactly where to begin, I found and installed this Cordova plugin from Shi11 called &lt;a href="https://github.com/shi11/RemoteControls"&gt;RemoteControls&lt;/a&gt;. It provides a couple ObjC methods that allow you to update the track meta and respond to any remote events that your app might recieve. &lt;/p&gt;
&lt;p&gt;The thing is, on the Objective C side Apple is very particular about &lt;a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/Remote-ControlEvents/Remote-ControlEvents.html"&gt;which views can and cannot recieve remote events&lt;/a&gt;. The procedure is simple, but in practice with Cordova it gets tricky:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Be the first responder.&lt;/strong&gt; The view or view controller that presents the multimedia content must be the first responder.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Turn on the delivery of remote control events.&lt;/strong&gt; Your app must explicitly request to begin receiving remote control events.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Begin playing audio.&lt;/strong&gt; Your app must be the “Now Playing” app. Restated, even if your app is the first responder and you have turned on event delivery, your app does not receive remote control events until it begins playing audio.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The last bullet is where the trouble started for me. To handle the streaming audio, I'm currently using &lt;a href="https://github.com/keosuofficial/cordova-audio-stream-plugin"&gt;Cordova Audio Stream&lt;/a&gt;. It leverages the &lt;a href="https://developer.apple.com/library/mac/documentation/AVFoundation/Reference/AVPlayer_Class/index.html"&gt;AvPlayer&lt;/a&gt; class to play the audio separately from the webview. &lt;/p&gt;
&lt;p&gt;Now, herein lies the problem. If you follow the documentation for RemoteControls, you will subscribe for remote events within your MainViewController.m, the controller for the WebView. However, since the audio is being instantiated and played from a totally different file within the plugins directory (CDVStream.m) the remote events never get sent to the app. I believe this is because the file that requests the remote events must also be the file that instantiates and plays the audio object. &lt;/p&gt;
&lt;p&gt;SO! If you're following, you can probably guess the solution to this problem. Instead of modifying the MainViewController.m I added the remote events subscription to CDVStream.m where the audio was being played from and voila! It works. &lt;/p&gt;
&lt;p&gt;Coming from a primarily web background, this totally wasn't intuitive for me at all, so hopefully this helps someone in the long run! &lt;/p&gt;
&lt;p&gt;Here's my version of my CDVStream.m, the same logic can easily be duplicated in any file that plays audio more than likely: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-objectivec"&gt;
/*
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements. See the NOTICE file
distributed with this work for additional information
regarding copyright ownership. The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied. See the License for the
specific language governing permissions and limitations
under the License.
*/

#import "CDVStream.h"
#import "RemoteControls.h"

@implementation CDVStream

@synthesize objAVPlayer;

- (void)create:(CDVInvokedUrlCommand*)command
{
    [self.commandDelegate runInBackground:^{
        CDVPluginResult* result = [CDVPluginResult resultWithStatus:CDVCommandStatus_OK];
        [self.commandDelegate sendPluginResult:result callbackId:command.callbackId];
    }];

}

- (void)startPlayingAudio:(CDVInvokedUrlCommand*)command
{
        // begin recieving remote events
        [[UIApplication sharedApplication] beginReceivingRemoteControlEvents];
        NSString* resourcePath = [command.arguments objectAtIndex:1];
        NSURL* resourceURL = [NSURL URLWithString:resourcePath];
        NSLog(@"Now Playing '%@'", resourcePath);
        if([self objAVPlayer] == nil){
            [self setObjAVPlayer:[[AVPlayer alloc] initWithURL:resourceURL]];
            [[self objAVPlayer] addObserver:self forKeyPath:@"status" options:0 context:nil];
        }else{
            [[self objAVPlayer] play];
        }
        return;
}
- (void)remoteControlReceivedWithEvent:(UIEvent *)receivedEvent
{
    // where to send the events when they are recieved
    [[RemoteControls remoteControls]
     receiveRemoteEvent:receivedEvent];
}

- (void) observeValueForKeyPath:(NSString *)keyPath 
                                ofObject:(id)object 
                                change:(NSDictionary  *)change 
                                context:(void *)context {

    if (object == [self objAVPlayer] &amp;&amp; [keyPath isEqualToString:@"status"]) {
        if ([self objAVPlayer].status == AVPlayerStatusReadyToPlay) {
            //Audio session is set to allow streaming in background
            AVAudioSession *audioSession = [AVAudioSession sharedInstance];
            [audioSession setCategory:AVAudioSessionCategoryPlayback error:nil];
            [[self objAVPlayer] play];
        }
        if ([self objAVPlayer].status == AVPlayerStatusFailed) {
            NSLog(@"Something went wrong: %@", [self objAVPlayer].error);
        }
    }
}


- (void)stopPlayingAudio:(CDVInvokedUrlCommand*)command
{
    [[self objAVPlayer] pause];
    // I don't want to stop recieving events when the user presses "pause" 
    // so I dont ever stop recieving events
    // I'm relying on the fact that music is no longer playing to halt the events
    //[[UIApplication sharedApplication] endReceivingRemoteControlEvents];

}

@end
&lt;/code&gt;
&lt;/pre&gt;</content><category term="Ionic"></category><category term="Ionic Framework"></category><category term="Hybrid Apps"></category><category term="Javascript"></category><category term="iOS"></category></entry><entry><title>A Quick Script to Make Your SSH-ing Easier</title><link href="https://connerswann.me/2015/01/a-quick-script-to-make-your-ssh-ing-easier.html" rel="alternate"></link><published>2015-01-21T15:50:00-08:00</published><updated>2015-01-21T15:50:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-01-21:/2015/01/a-quick-script-to-make-your-ssh-ing-easier.html</id><summary type="html">&lt;p&gt;At work, I find myself SSH-ing into at least five different machines a day. I use a password manager, so my workflow looks like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;ssh conner@myserver.com &lt;/li&gt;
&lt;li&gt;Grab my password from password manager&lt;/li&gt;
&lt;li&gt;paste password into terminal&lt;/li&gt;
&lt;li&gt;if you messed up GOTO 1&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;This gets really tedious after …&lt;/p&gt;</summary><content type="html">&lt;p&gt;At work, I find myself SSH-ing into at least five different machines a day. I use a password manager, so my workflow looks like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;ssh conner@myserver.com &lt;/li&gt;
&lt;li&gt;Grab my password from password manager&lt;/li&gt;
&lt;li&gt;paste password into terminal&lt;/li&gt;
&lt;li&gt;if you messed up GOTO 1&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;This gets really tedious after the 20th time, so I wrote a quick Bash script that leverages the handy &lt;a href="http://manpages.ubuntu.com/manpages/precise/man1/ssh-copy-id.1.html"&gt;ssh-copy-id&lt;/a&gt; command. &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-bash"&gt;
##########################################
# This script relies on the package "ssh-copy-id". It's on all your major package 
# managers. This script was written for an OSX environment, but there's no reason 
# why it wouldn't work on other unix-ey platforms. 
#
# WARNING: Use this at your own risk, I'm not responsible if you break something by running it.
#
#   The following happens when this script runs.
#   .5. Data is gathered.
#   1. Keys are generated and named based on the short name you supply in ~/.ssh/.
#   2. The private key is sent to the server with ssh-copy-key using the port specified. 
#   3. The pertinent information is added to your ssh-config (if you haven't made one, it'll make one for you).
#   
#   You can now ssh into your server using "ssh server_short_name"!
#
# 08/20/2014 http://connerswann.me

server_host_name=""
server_short_name=""
server_user=""
server_port=""
ssh_config=~/.ssh/config

clear 

if [ -z "$(which ssh-copy-id)" ]; then
    echo "Dude, you need 'ssh-copy-id' installed. Dying..."
    exit 1
fi

echo "Sup, let's save some SSH keys."

while [[ ! $confirm =~ ^([yY][eE][sS]|[yY])$ || $confirm == "" ]]; do
    echo 
    echo "Enter the server host name: "
    read server_host_name
    echo "Enter a short name for the server: "
    read server_short_name
    echo "Enter your username on the external server: "
    read server_user
    echo "Which port should I connect to? (just hit ENTER for default port 22)"
    read server_port
    if [ -z "$server_port" ] ; then server_port=22; fi #map empty server_port -&gt; 22

    echo "Please verify that this is correct."
    echo "Short Name -&gt; $server_short_name"
    echo "Host Name -&gt; $server_host_name"
    echo "Server User -&gt; $server_user"
    echo "Server Port -&gt; $server_port"
    echo "Confirm? (Y/n)"
    read confirm 
    if [ -z "$confirm" ] ; then confirm="Y"; fi #map empty string to -&gt; "Y"
done

clear
echo "Passing to ssh-keygen..."
ssh-keygen -t rsa -f ~/.ssh/$server_short_name 

echo "I hope that worked "
echo "Press ENTER to continue."
read confirm

cd ~/.ssh/

ssh-copy-id -p $server_port -i $server_short_name.pub $server_user@$server_host_name

touch $ssh_config

echo &gt;&gt;$ssh_config
echo "Host $server_short_name" &gt;&gt;$ssh_config
echo "Hostname $server_host_name" &gt;&gt;$ssh_config
echo "User $server_user" &gt;&gt;$ssh_config
echo "Port $server_port" &gt;&gt;$ssh_config
echo "IdentityFile ~/.ssh/$server_short_name" &gt;&gt;$ssh_config

echo "Contents of SSH Config File:"
echo 
cat $ssh_config
echo 
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;The comments really say it all, but here's the TL;DR:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It collects information about the server you're configuring&lt;/li&gt;
&lt;li&gt;It generates RSA keys for password-less identification with ssh-keygen&lt;/li&gt;
&lt;li&gt;It sends those keys to the server using ssh-copy-id and saves all the information to your SSH profile&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, my workflow looks like this: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt; ssh myserver&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There you have it! If you've got any questions as to how this works or implementing it, shoot me a &lt;a href="http://twitter.com/yourbuddyconner"&gt;tweet&lt;/a&gt;!&lt;/p&gt;</content><category term="MacOS"></category><category term="MacOS"></category><category term="BASH"></category><category term="Script"></category><category term="Fixes"></category></entry><entry><title>Internet of Things: Connecting my WiFi Arduino and PHP</title><link href="https://connerswann.me/2015/01/internet-of-things-connecting-my-wifi-arduino-and-php.html" rel="alternate"></link><published>2015-01-01T21:04:00-08:00</published><updated>2015-01-01T21:04:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2015-01-01:/2015/01/internet-of-things-connecting-my-wifi-arduino-and-php.html</id><summary type="html">&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;In a &lt;a href="http://connerswann.me/playing-with-the-adafruit-cc3000-wifi-shield/"&gt;previous post&lt;/a&gt; I discussed the Adafruit CC3000 WiFi shield and how I got the hardware and libraries installed. In this post I'm going to show you how I got my WiFi Arduino talking to a simple PHP script. &lt;/p&gt;
&lt;p&gt;First, I think it's helpful to visualize exactly what's …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;In a &lt;a href="http://connerswann.me/playing-with-the-adafruit-cc3000-wifi-shield/"&gt;previous post&lt;/a&gt; I discussed the Adafruit CC3000 WiFi shield and how I got the hardware and libraries installed. In this post I'm going to show you how I got my WiFi Arduino talking to a simple PHP script. &lt;/p&gt;
&lt;p&gt;First, I think it's helpful to visualize exactly what's going on in these sorts of interactions. The handy diagram below is a rough sketch of what's talking to what (it's actually much more complicated, but the code I wrote pertains to these actors):
&lt;/br&gt;
&lt;img alt="Arduino WiFi Diagram" src="https://connerswann.me/images/2015/internet-of-things-arduino-diagram.png"&gt;&lt;/p&gt;
&lt;p&gt;The Arduino first connects to the WiFi access point through the CC3000, piping the credentials and configuration options through one of several pins on the Arduino to the CC3000 hardware. The CC3000 handles the logistics of sending and revieving messages from the router, returning the results to the Arduino. &lt;/p&gt;
&lt;p&gt;Then, through the magic of TCP, these messages that are sent from the Arduino are recieved and interpreted by my server running PHP. For this exercise, I used the &lt;a href="http://www.slimframework.com/"&gt;Slim PHP Framework&lt;/a&gt; to create two URL Routes (more on that later) that the arduino can send requests to -- one that allows it to set a value and one that simply responds with the current value. These values are collected and stored in a MySQL database that is queried by PHP.  &lt;/p&gt;
&lt;p&gt;Once the initial request is interpreted by the PHP script, a response is generated and sent back via TCP to the Arduino for it to interpret. &lt;/p&gt;
&lt;p&gt;As I hope you can see, this sort of interaction is highly extensible and can be leveraged to preform all sorts of duties. For example, you could report values to a server and have the server catch them and store them in a database or periodically check for messages and display any that it recieves. Also, with a WiFi shield the communication is totally wireless!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The Arduino Code&lt;/h2&gt;
&lt;p&gt;Now it's time for the code, however it does require a little explanation. I used an command line application called &lt;a href="https://www.biicode.com/"&gt;"bii"&lt;/a&gt; to do this project. It's a dependancy manager that keeps all your libraries in one place and fetches them from the internet if they're not there locally.  At compile time, bii reads your main C++ file, collects all of the includes, and links them. It's very handy at the expense of having random includes in your file. It is also capable of compiling your Arduino source files and uploading them to your board, a definite plus.&lt;/p&gt;
&lt;p&gt;I did this because doing advanced interactions via HTTP on the Arduino is actually really hard and requires a lot of string manipulation. HTTP responses in this context are all plain text streams and the CC3000 library doesn't have any capability to parse out any information in the responses it recieves. The Bii repository had a &lt;a href="https://www.biicode.com/lasote/lasote/arduino_http/master/25/readme.md"&gt;nice library created by a guy named "Lasote"&lt;/a&gt; that did just that, so I decided to roll with it. &lt;/p&gt;
&lt;p&gt;Here's an example HTTP Response so you have an idea of what you need to parse through and why this client interface is necessary: &lt;/p&gt;
&lt;pre&gt;
&lt;code class="language-http"&gt;
Date: Thu, 01 Jan 2015 21:27:23 GMT
Server: Apache/2.4.7 (Ubuntu)
X-Powered-By: PHP/5.5.9-1ubuntu4.5
Vary: Accept-Encoding
Content-Encoding: gzip
Content-Length: 87
Keep-Alive: timeout=5, max=100
Connection: Keep-Alive
Content-Type: text/html
{"id":"4","value":"99","reason":"To test if the arduino can decode json"}
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Being able to print out &lt;code&gt;ret_code&lt;/code&gt; and the raw response body in &lt;code&gt;response_buffer&lt;/code&gt; (both variables populated by this interface) as opposed to having to parse through the entire string above is pretty handy.&lt;/p&gt;
&lt;p&gt;Now, without further ado, here's my C++ code for the arduino:&lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-c"&gt;
// Arduino symbols and Bindings
#include "Arduino.h"
#include "ntruchsess/arduino_uip/uipethernet.h"
// client interface for HTTP communication
#include "lasote/arduino_http/http_client.h"

// a processor to decode JSON
#include "lasote/stream_processor/stream_json_processor.h"
#include "lasote/stream_recorder/stream_recorder.h"

//For cc3000 adapter
#include &lt;adafruit/cc3000_library/adafruit_cc3000.h&gt;
#include "diego/ardunet/cc3000client.h"
#include "diego/ardunet/cc3000utils.h"

#define WLAN_SSID       "Atlantis"        // cannot be longer than 32 characters!
//#define WLAN_PASS       "xxxxxxxx"
char WLAN_PASS[] = {0x69, 0x69, 0x69, 0x69, 0x69, 0x00}; 
// Security can be WLAN_SEC_UNSEC, WLAN_SEC_WEP, WLAN_SEC_WPA or WLAN_SEC_WPA2
#define WLAN_SECURITY   WLAN_SEC_WEP

// These are the interrupt and control pins
#define ADAFRUIT_CC3000_IRQ   3  // MUST be an interrupt pin!
// These can be any two pins
#define ADAFRUIT_CC3000_VBAT  5
#define ADAFRUIT_CC3000_CS    10


using namespace lasote;

//create CC3000 instance, and an SDK compatible Client
Adafruit_CC3000 cc3000 = Adafruit_CC3000(ADAFRUIT_CC3000_CS,
        ADAFRUIT_CC3000_IRQ, ADAFRUIT_CC3000_VBAT, SPI_CLOCK_DIVIDER);

CC3000Client client(cc3000);

char response_buffer[150];
// This processor simply writes the response to a buffer
StreamRecorderProcessor response_recorder(response_buffer, 150); 
HttpClient http_client(client, Serial);

// the keys of the values I'd like to fetch from the response
//const char* queries[] = {"id", "value", "reason"};
//                  processor(ArrayofQueries, nQueries, MaxNameSize, MaxValueSize, MaxTraceSize)
//StreamJsonProcessor processor(queries, 3, 10, 10, 10);
// the setup routine runs once when you press reset:
void setup() {
    // open the serial connection to report messages
    Serial.begin(9600);
    Serial.println("************* SETUP ****************");
    // begin the wifi connection using the cc3000 interface
    startConnection(cc3000, WLAN_SSID, WLAN_PASS, WLAN_SECURITY);

    // add a HTTP Response processor
    // this will take the response stream and process it depending on
    // which processor we use. (i.e. JSON or just a raw buffer)
    // http_client.processors.add_item(&amp;processor);
    http_client.processors.add_item(&amp;response_recorder);

}

// the loop routine runs over and over again forever:
void loop() {

    delay(20000);
    Serial.println("--- SENDING POST---");
    //Reset processors and custom headers
    http_client.reset(); 
    //We can add custom headers to the request if we wanted
    //http_client.add_custom_header("Content-Type", "application/json");

    // Send an HTTP GET request for the value
    int sent = http_client.get("condejo.org", "/api/wifitest.php/getvalue", 80);
    // Evaluate the return code from the HTTP response
    short int ret_code = http_client.receive();
    Serial.println(ret_code);
    if(sent){
        if(ret_code){
            if(ret_code == 200){
                Serial.println("Code 200 OK!");
                // for the JSON processor if we were using it
                // if(processor.finished()){
                //     Serial.print("Reading ID: ");
                //     Serial.println(processor.results[0]);
                //     Serial.print("Reading Value: ");
                //     Serial.println(processor.results[1]);
                //     Serial.print("Reason: ");
                //     Serial.println(processor.results[2]);
                // }
                // else{
                //     Serial.println("Return JSON not detected or queries not found!");
                // }
                Serial.println(response_buffer);
            }
            else{
                // PHP didn't like the request 
                Serial.print("Http Error:");
                Serial.println(ret_code);
            }
        }
        else{
            // TCP Broke and we didn't get a response to our request
            Serial.println("Error receiving!");
        }
    }
    else{
        // Couldn't send the initial request in the first place
        Serial.println("ERROR CONNECTING... retrying");
    }
}
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;On first run, this program runs a subroutine that initializes critical objects and connects to the configures WiFi access point (in this example, it's called Atlantis). &lt;/p&gt;
&lt;p&gt;It then enters a loop, making a request to the server once every ten seconds. I simply have the arduino report this response over Serial, however you can do much more with it. For example, the response can be programmatically used as a trigger to preform advanced functions like turning LED's on/off, sending a text message, or even sending a new HTTP request to a different server. &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The PHP Code&lt;/h2&gt;
&lt;p&gt;As I already discussed above, the PHP is designed to recieve, interpret, and respond to the Arduino's HTTP requests. &lt;/p&gt;
&lt;p&gt;For this exercise, I decided to work with &lt;a href="http://www.slimframework.com/"&gt;Slim PHP Framework&lt;/a&gt; because it makes doing this sort of thing trivial, whereas with raw PHP it would be much more involved (read: more code). In this context I'm only utilizing Slim's easy-to-use URL router, however there's plenty more to be had including a templating system and a debugging interface (this last one is immensely useful). &lt;/p&gt;
&lt;p&gt;Here's the code: &lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-php"&gt;
&lt; ?php

require 'libs/Slim/Slim.php';
require_once 'dbHandler.php';

\Slim\Slim::registerAutoloader();

$app = new \Slim\Slim();

// create a route at wifitest.php/getvalue
// when someone makes an HTTP GET request here, this function will be called
$app-&gt;get('/getvalue', function() {
    // dbHandler is a piece of code I picked up along the way that encapsulates
    // often repeated SQL commands. I've modified it pretty heavily, and it's probably
    // its own post at this point.
    $db = new DbHandler();
    // grab the record with the highest ID
    $record = $db-&gt;getOneRecord("SELECT * FROM wifitest ORDER BY reading_id DESC");
    // create a new response object and populate it
    $response["id"] = $record["reading_id"];
    $response["value"] = $record["reading_value"];
    $response["reason"] = "To test if the arduino can decode json"; 
    // convert it to JSON and send it back to the client
    echo json_encode($response);
}); 

// create a route at wifitest.php/setvalue
$app-&gt;get('/setvalue', function() use ($app){
    // grab the request parameter from the Slim app object
    $value = $app-&gt;request()-&gt;params('value');
    $db = new DbHandler();
    if ($value){
        // create a new data object
        $newvalue["reading_value"] = $value;
        // insert that ish to the DB 
        $result = $db-&gt;insertIntoTable($newvalue, array('reading_value'), 'wifitest');
        if ($result != NULL) {
            // let the client know the insert was a success
            $response["message"] = "Value saved!";
            $response["code"] = 1;
            echo json_encode($response);
        }
        else{
            // let the user know the insert failed
            $response["message"] = "Value not saved.";
            $response["code"] = 0;
            echo json_encode($response);
        }
    }
});

$app-&gt;run();
?&gt;
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;If you've got any questions or comments about how this works, feel free to shoot me an email or Tweet! Thanks for reading. &lt;/p&gt;</content><category term="Arduino"></category><category term="Wi-Fi"></category><category term="Arduino"></category><category term="PHP"></category><category term="REST"></category><category term="API"></category></entry><entry><title>Arduino: Introducing The Adafruit CC3000 WiFi Shield</title><link href="https://connerswann.me/2014/12/arduino-introducing-the-adafruit-cc3000-wifi-shield.html" rel="alternate"></link><published>2014-12-29T00:45:00-08:00</published><updated>2014-12-29T00:45:00-08:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2014-12-29:/2014/12/arduino-introducing-the-adafruit-cc3000-wifi-shield.html</id><summary type="html">&lt;p&gt;I recently got my hands on an &lt;a href="http://www.adafruit.com/product/1491"&gt;Adafruit CC3000 WiFi Shield&lt;/a&gt;. From the Adafruit product page: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"The CC3000 hits that sweet spot of usability, price and capability. It uses SPI for communication (not UART!) so you can push data as fast as you want or as slow as you want …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;I recently got my hands on an &lt;a href="http://www.adafruit.com/product/1491"&gt;Adafruit CC3000 WiFi Shield&lt;/a&gt;. From the Adafruit product page: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"The CC3000 hits that sweet spot of usability, price and capability. It uses SPI for communication (not UART!) so you can push data as fast as you want or as slow as you want. It has a proper interrupt system with IRQ pin so you can have asynchronous connections. It supports 802.11b/g, open/WEP/WPA/WPA2 security, TKIP &amp;amp; AES. A built in TCP/IP stack with a "BSD socket" interface. TCP and UDP in both client and server mode, up to 4 concurrent sockets."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In not so many words, it's a piece of hardware that can be used to connect your Arduino to a WiFi access point to send and recieve data wirelessly. It's a pretty robust piece of hardware, and it's perfect to connect my "Internet of Things" projects to the internet. &lt;/p&gt;
&lt;p&gt;To get my setup working, I stepped through the &lt;a href="https://learn.adafruit.com/adafruit-cc3000-wifi"&gt;Adafruit Documentation on the CC3000&lt;/a&gt; which is complete and very thorough. &lt;/p&gt;
&lt;p&gt;Before you can start using this board, you have to solder on the headers so you can plug it directly into the Arduino. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Since I'm more of a software guy, I won't detail the actual process of soldering the headers, but if you're looking for instructions &lt;a href="https://learn.adafruit.com/adafruit-cc3000-wifi/assembly-and-wiring"&gt;Adafruit has you covered&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Arduino with WiFi Shield" src="https://connerswann.me/2014/12/arduino-wifi-shield-1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;To get it running, you have to add the shield's code libraries to your &lt;code&gt;arduino_sketch_location/libraries/&lt;/code&gt; folder, the library is available for download along with installation instructions &lt;a href="https://learn.adafruit.com/adafruit-cc3000-wifi/cc3000-library-software"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From there, all I did was pop the shield into place on top of the Arduino and load up the 'buildtest' example file [1]. &lt;/p&gt;
&lt;p&gt;I got an output that looked like this: &lt;/p&gt;
&lt;pre&gt;
&lt;code class="language-none"&gt;
Hello, CC3000!

RX Buffer : 131 bytes
TX Buffer : 131 bytes
Free RAM: 1221

Initialising the CC3000 ...
Firmware V. : 1.24
MAC Address : 0x08 0x00 0x28 0x57 0xA4 0xF5
Networks found: 10
================================================
SSID Name    : AtlantisBack
RSSI         : 63
Security Mode: 1

SSID Name    : DIRECT-c0[BD]EM59
RSSI         : 55
Security Mode: 3

SSID Name    : Someone's Wi-Fi Network
RSSI         : 37
Security Mode: 3

SSID Name    : Atlantis
RSSI         : 55
Security Mode: 1

================================================

Deleting old connection profiles

Attempting to connect to Atlantis
Connected!
Request DHCP

IP Addr: 192.168.1.134
Netmask: 255.255.255.0
Gateway: 192.168.1.1
DHCPsrv: 192.168.1.1
DNSserv: 76.14.0.8
www.adafruit.com -&gt; 207.58.139.247

Pinging 207.58.139.247...5 replies
Ping successful!


Closing the connection
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;This is just a really basic example so I could prove to myself that I could get the shield working. However, my end-goal is to get the board speaking to a server and reporting some values, similarly to what I did in a &lt;a href="http://connerswann.me/arduino-an-internet-of-things/"&gt;previous post with an Ethernet shield&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Stay tuned!&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;The documentation actually says that you shouldn't be able to run the Arduino and the CC3000 shield off of your computer's USB port due to power consumption being too high, however I wasn't able to verify this. It might be a problem as you get into more advanced things and require more of the Arduino's pins to be powered.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Arduino"></category><category term="Arduino"></category><category term="IOT"></category><category term="HTTP"></category><category term="Ethernet Shield"></category></entry><entry><title>Verve: Death-Drink and Money-Waster or Solid Gold Ambrosia of the Gods?</title><link href="https://connerswann.me/2014/07/verve-death-drink-and-money-waster-or-solid-gold-ambrosia-of-the-gods.html" rel="alternate"></link><published>2014-07-31T16:32:00-07:00</published><updated>2014-07-31T16:32:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2014-07-31:/2014/07/verve-death-drink-and-money-waster-or-solid-gold-ambrosia-of-the-gods.html</id><summary type="html">&lt;p&gt;&lt;em&gt;This is a blog post which was originally published  in 2012 on my old blog at condejo.org. I believe that the information within holds true to this day, but there is a posibility that my data is no longer correct. Please feel free to send me an email with …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This is a blog post which was originally published  in 2012 on my old blog at condejo.org. I believe that the information within holds true to this day, but there is a posibility that my data is no longer correct. Please feel free to send me an email with corrections if there are any glaring inaccuracies.&lt;/em&gt;&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;img class="aligncenter  wp-image-307" title="vervearticle" src="http://condejo.org/wp-content/uploads/2012/09/vervearticle.png" alt="" width="736" height="160" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span style="font-size: large;"&gt;Disclaimer:All of the following non-publicly-available names are fictitious, but the words said are 100% accurate and true to form. Any webpages that I will refer to have been copied on Monday, September 17, 2012 and are locally hosted and are available as images.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style="font-size: medium;"&gt;[Quick Edit: This is in response to some linking of this post under the guise that it's a "false understanding on what they're doing as a &lt;em&gt;movement&lt;/em&gt;." While I respect almost any attempt to make some money by selling legal goods, calling Verve a social "movement" is just silly, unless you're referring to the imminent bowel movement you'll be partaking in after ingesting three cans of the highly caffeinated soft drink. It's a beverage manufacturer, not a political party...  I'll let you decide for yourself, but that's my take on it.]&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Okay. Recently some former peers of mine as well as some sketchy people on campus  have been bellowing to the heavens about this "new" energy drink, catchily named "Verve!" There were some pretty unbelievable things being said about this miracle product that could allow you to take [sic] "a nap and waking up and seeing that I made money while I was sleeping." - Joey Jobmeister&lt;/p&gt;
&lt;p&gt;Now, I'm not very old, but I know the mantra that states "If it's too good to be true, it usually is." I was naturally skeptical and did the due-diligence to glean more information, but what really made me giggle was the following statement about what Verve!'s affiliate program can do for you:&lt;/p&gt;
&lt;blockquote&gt;What I also know is none of that shit matters because 15 year olds are making more than there damn principals and driving free cars before they even have their licenses. So idgaf who's the smartest or toughest it's pretty much who just wants to be in the right place in the right time and thanks to Holly Helpful you all have that opportunity now. Take it or leave it. - Joey Jobmeister&lt;/blockquote&gt;
&lt;p&gt;Okay. You've got my attention. Let's see what Verve! and it's manufacturer Vemma can do, and wether or not it's as true-to-form as Joey over here is saying it is.&lt;/p&gt;
&lt;p&gt;First, a history lesson. What is Vemma and what's so amazing about their product(s)?&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="font-size: x-large;"&gt;&lt;strong&gt;Vemma - The Alleged Poor/Sick/Unhealthy/Unfit Man's Savior&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;First off, I'd like to mention that their company &lt;a title="Company Bio - Vemma" href="http://www.vemma.com/our-story/"&gt;bio&lt;/a&gt; reads like a self-help ad out of a cheap tabloid, but I'll ignore that for the sake of science or something.&lt;/p&gt;
&lt;p&gt;Vemma is based in Scottsdale, Arizona and headed up by the founder and CEO BK Boreyko. From the get-go in their description, the company alleges that their core mission is to help consumers by "enhancing their well-being" and to offer "an income stream to people who introduce others to a product line they believe in." Those are some pretty genuine-sounding statements from a health-food company, but it doesn't make me want to buy their product. Not yet anyway.&lt;/p&gt;
&lt;p&gt;I immediately changed my mind when they insinuated that Vemma products can indirectly cure bankruptcy  and divorce. I shit you not.&lt;/p&gt;
&lt;blockquote&gt;Considering that the number-one reason for personal bankruptcy is illness, and the number-one reason for divorce is financial difficulties, the need Vemma can fill is incredibly important. It's really hard to feel like you're living the life you deserve when you're sick or unhappy.&lt;/blockquote&gt;
&lt;p&gt;If there's any other way to read that, please let me know, because that's a tall order for a dietary supplement manufacturer...&lt;/p&gt;
&lt;p&gt;I'll spare you the rest of the masturbatory dialogue, but their basic claims are as follows:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;Their compensation plan for affiliates is "strong, generous and &lt;em&gt; charitable.&lt;/em&gt;"&lt;/li&gt;
    &lt;li&gt;Their product is "ultra-premium"&lt;/li&gt;
    &lt;li&gt;Their product will help you be healthier and happier.&lt;/li&gt;
    &lt;li&gt;Their product is for people who like "helping others."&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I only really want to address their "charitable" affiliate program for the time being, as that was the source of the argument which lead me to write this in the first place. I'll give them the benefit of the doubt that their absolutely ridiculous claims of "Ultra-Premium" product are true.&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="font-size: x-large;"&gt;&lt;strong&gt;Vemma and Verve! - Early Retirement For Teens and Infants or Something&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style="text-align: left;"&gt;Vemma has an incredibly well-worded and enticing pitch to potential affiliates, I'll give them that. Just look at the friggin &lt;a title="Screenshot of &amp;quot;Opportunities&amp;quot;" href="http://condejo.org/images/Verve%20Article/Vemma%20Business%20Model%20-%20The%20Vemma%20Opportunity.png" target="_blank"&gt;landing page&lt;/a&gt; when you click on "Opportunity" on the official website. In case you don't want to go through the trouble of clicking on it, I'll describe it for you.&lt;/p&gt;
&lt;p style="text-align: left;"&gt;An image dominates the page which depicts what appears to be a stock image of a male and a female in a convertible. Superimposed over the image in the Vemma trademark orange text are the words "If You Could Not Fail, How Big Would You Dream?" Underneath this admittedly heart-lifting image are the words, "Achieve Your Dreams with Our Rewarding Business Strategy."&lt;/p&gt;
&lt;p style="text-align: left;"&gt;Now if that's not enough well-placed idealism, the entire "Opportunity" section of their site is filled with intentionally vague and cliche words of encouragement like,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p style="text-align: left;"&gt;Unlimited possibilities! We don't place any barriers ahead of you; we let you determine your own success.&lt;/p&gt;
&lt;p style="text-align: center;"&gt;or&lt;/p&gt;
&lt;p style="text-align: left;"&gt;So if your dream is to run your own business, get out of debt, earn a secondary source of income, exchange the traditional 9 to 5  for your own schedule, or all of the above, Vemma's business opportunity can help you achieve it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p style="text-align: left;"&gt; I'll move on from the obvious to the not-so-obvious forms of borderline misleading information. If you navigate to "How to Get Paid" you'll be confronted with a video that hypothetically describes how you can make money with Vemma. While I admit it does that, it also feeds information to the viewer that requires a disclaimer that reads "Results not Typical." Claims like, "We have a Game Plan to get you earning $500, $5,000, or even $50,000 per  month!**"&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=yK7GbjDk0iE"&gt;Video&lt;/a&gt;&lt;/p&gt;
&lt;p style="text-align: left;"&gt;Mr. Boreyko likes to use large fantastical language all throughout the video, while &lt;em&gt;almost never &lt;/em&gt;actually saying anything of real substance. The entire script is worded in such a way that it appears to insinuate a bunch of cool things that are available only if you work for Vemma. Allegations that the "Game Plan" can give the &lt;em&gt;&lt;span style="text-decoration: underline;"&gt;opportunity&lt;/span&gt;&lt;/em&gt; to be &lt;em&gt;&lt;span style="text-decoration: underline;"&gt;profitable&lt;/span&gt;&lt;/em&gt; fast and also be driving a new BMW in 90 days. Yes. All in the same sentence.&lt;/p&gt;
&lt;p style="text-align: left;"&gt;Let's break that down really quick, it's important to see the wording here that is meant to be misinterpreted. This "Game Plan" will give you the opportunity to make at least one dollar more than you spent, and ALSO will give you the CHANCE to enter a CONTEST to win a new car.&lt;/p&gt;
&lt;p style="text-align: left;"&gt;Now, being misleading to total strangers is something I definitely disagree with, however where I personally begin to have serious reservations about companies like Vemma is the point when they request you take advantage of the trust you've built and use the same almost misleading, superfluous language with your friends and family.&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;strong&gt;&lt;span style="font-size: x-large;"&gt;The "Game Plan" Also Known As: "Rationale to Milk People That Trust You Out Of Their Money"&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p style="text-align: left;"&gt;In the video cited above, it assumes you are starting your own pyramid from scratch, so step one is to get people to be beneath you.&lt;/p&gt;
&lt;p style="text-align: left;"&gt;&lt;span style="font-size: medium;"&gt;&lt;strong&gt;Step 1: Convince Suckers to Sell Beneath You&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style="text-align: left;"&gt;&lt;span style="font-size: medium;"&gt;First, you have to find people that "see what you see" and sign them up underneath you. The video suggests three, but their business model only requires two (I guess you have to just drop one, whoops!).  With those people, you then &lt;strong&gt;&lt;em&gt;buy&lt;/em&gt;&lt;/strong&gt; one or more "builder packs," please correct me if I'm wrong in the comments or via e-mail, but basically you're fronting about $500.00 or more for a couple of cases of their golden health nectar. Look here at the &lt;a title="Verve Builder Packs" href="http://condejo.org/images/Verve%20Article/Vemma%20Store%20-%20Verve%20Builder%20Packs.png"&gt;screenshot&lt;/a&gt; or even their &lt;a title="Verve Store" href="https://www.vemma.com/store/index.cfm?fuseAction=dspProductVerveStore"&gt;store&lt;/a&gt; which is rampant with unicode errors. Accordinng to the video at that point you should have made "about $700" yet he never clarified exactly what you'd have sold at that point. Is it one $500 set of Verve! Is it two? Seven? &lt;/span&gt;&lt;/p&gt;
&lt;p style="text-align: left;"&gt;Okay, I apologize to those of you who didn't read this for the math, but I wanted to put numbers to the words.&lt;/p&gt;
&lt;p style="text-align: left; padding-left: 30px;"&gt;In one $499.95 "Verve! Builder Pack" there are:&lt;/p&gt;

&lt;ul&gt;
    &lt;li&gt;72 cans of Verve Energy Drink (8.3-oz. can)&lt;/li&gt;
    &lt;li&gt;48 cans of Verve Zero Sugar Energy Drink (8.3-oz. can)&lt;/li&gt;
    &lt;li&gt;48 bottles of Verve Energy Shot (2-oz. bottles)&lt;/li&gt;
&lt;/ul&gt;
&lt;p style="padding-left: 30px;"&gt;That's a total of 120, 8.3-oz. cans which I would sell at the same price, and 48 2-oz. shots which obviously you'd sell for less than the cans.&lt;/p&gt;
&lt;p style="padding-left: 30px;"&gt;The only way I thought to price it all out was to simply price it by the Fluid Ounce, because according to Vemma all their stuff if the same and it's being sold wholesale anyways.&lt;/p&gt;
&lt;p style="padding-left: 30px;"&gt;That's 996 Fluid Ounces in the cans and 96 in the bottles. Divided, the liquid is distributed 91.2% to the cans and 7.8% to the bottles. The final cost coming to $455.95 of your investment going into the cans and $40 going into the little bottles. About $3.80 a can and $0.83 a bottle.&lt;/p&gt;
&lt;p style="padding-left: 30px;"&gt;With a can of Red Bull, Verve!'s main competitor, costing about &lt;a title="Red Bull Data" href="http://energy-drinks.findthebest.com/l/156/Red-Bull-Energy-Drink" target="_blank"&gt;$2.99 a can&lt;/a&gt;, that doesn't leave a whole lot of room for competition.  Let's say that since Verve! is "Ultra-Premium" you're willing to pay at most 50% more for it, for a total of $4.50 a can. Assuming you are able to sell every single can, and don't drink any of them in the process, you'd net $540.00 from the cans, a profit of $84.05.&lt;/p&gt;
&lt;p style="padding-left: 30px;"&gt;Moving onto the bottles. The average 5-Hour Energy shot costs about &lt;a title="5-Hour Energy Shots Info" href="http://energy-drinks.findthebest.com/l/6/5-Hour-Energy" target="_blank"&gt;$1.50 a bottle&lt;/a&gt;, and again, assuming Verve! has the ability to cure all the ills of the world like Divorce and Bankruptcy, you're willing to pay 50% more for a taste. That's $2.25 a bottle, but let's round down to $2.00 for simplicity's sake. Again, assuming you are able to sell all 48 bottles, you've just collected $96.00, a $56.00 profit.&lt;/p&gt;
&lt;p style="padding-left: 30px;"&gt;Together, you've made a profit of $140.05, off of your original $500 investment, and if you're paying attention to their shipment schedule you only have a month before your next shipment comes.&lt;/p&gt;
&lt;p style="padding-left: 30px;"&gt;To meet the original estimate of $700 of profit, you'd have to sell 5 full cases of the stuff with an initial investment of $2,500.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2: Help Your Minions Out&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yup, you guessed right, as the people underneath you sell stuff, you get a kickback from their profits. At this point, you probably don't have to do much selling anymore because if your minions are doing what they're supposed to be doing you can just coast along on the percentage of their profits. You're basically like a Meth lord with dealers peddling small amounts of crystal on the streets of New Mexico and instead of Vemma your supplier is a man known only as Heisenberg.&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="color: #000000; font-size: x-large;"&gt;&lt;span style="line-height: 36px;"&gt;&lt;strong&gt;So... Now What?&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style="text-align: left;"&gt;To me, all this hardly sounds feasable, let alone sound judgement for someone who is   several steps down the "pyramid." Ah, you may be wondering, "What are you talking about? Pyramid? Like a Pyramid Scheme?" To that I answer, yes, exactly like a pyramid scheme. Refer to the following picture (click for a larger version):&lt;/p&gt;
&lt;p&gt;&lt;a href="http://condejo.org/images/Verve%20Article/mlm.gif"&gt;&lt;img class="aligncenter" style="line-height: 18px; font-size: 12px;" title="MLM" src="http://condejo.org/images/Verve%20Article/mlm.gif" alt="" width="196" height="265" /&gt;&lt;/a&gt;&lt;br&gt;This is an graphic representation of your basic Multi-Level-Marketing (or MLM from now onward) scheme. Assume for a moment that you are Tuco, the Verve! pusher at the top of the chart. You have your two lackies below you, Slim Jim and Gordo, and you give them strict instructions to recruit people beneath them to sell product with the condition that you get 10% of everybody's profits who sells beneath you.&lt;/p&gt;
&lt;p&gt;You get rich once the people stream in to get a taste of the drink that can purportedly cure cancer or something, and even richer as more and more people want to sell it for you. However, it's a pyramid, and profits only go one direction. Up.&lt;/p&gt;
&lt;p&gt;Eventually, the market becomes so inundated with people peddling Verve! and curing all the ills of humanity that nobody wants to buy it anymore, and the pyramid collapses. However you Mr. Tuco, are now fantastically rich and the people stuck with cases upon cases of Verve! are now stuck trying to figure out what the hell to do with all of their miracle water.&lt;/p&gt;
&lt;p&gt;Now, there is a difference between this and the illegal "pyramid" schemes. The illegal variety specifically doesn't have a product to sell, making it fraud. Instead of a tangible product, the illegal kind asks for an initial investment and then gives payments for recruiting more people (in the same fashion as most MLM strategies I might add) until there are so many people that it's impossible to keep track of anything and it collapses.  Although different, the basic defining point of a "pyramid" scheme remains the same and that's the unstable pyramid structure it produces.&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;strong&gt;&lt;span style="font-size: x-large;"&gt;The Bottom Line&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p style="text-align: left;"&gt; Can you make a significant amount money with Vemma? Yes, most definitely. Will you? The answer might surprise you. It's statistically &lt;strong&gt;INCREDIBLY UNLIKELY&lt;/strong&gt; that you or anyone else will make any large amount of money, with about 84% of affiliates having an annual income of less than $3,071.51. Even more miraculous, 73% make less than $1,000 annually.&lt;/p&gt;
&lt;p style="text-align: left;"&gt;For your information, if you were listening and thinking that the "opportunity" to make "$500, $5,000, 0r even $50,000 each month," was a easy-to-achieve goal, I hate to break it to you,  it's not. Less than 8% make the lowest of the list, $500 a month, about .2% make $5,000, and a grand total of zero makes $50,000.&lt;/p&gt;
&lt;p style="text-align: left;"&gt;&lt;a title="Vemma Income Disclosure" href="http://www.vemma.com/backoffice/pdf/2011-income-disclosure.pdf" target="_blank"&gt;Sorry. Look for yourself.&lt;/a&gt;&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;strong&gt;&lt;span style="font-size: x-large;"&gt;&lt;span style="line-height: 36px;"&gt;Any&lt;/span&gt;&lt;span style="line-height: 36px;"&gt; Questions?&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So that's it. Now you have a  (hopefully) good, simple to understand layman's guide to Verve! and now you're able to make an informed decision about wether or not to get involved with this company.&lt;/p&gt;
&lt;p&gt;If you have any questions, or suggestions for things to add to this I'd be happy to listen. There's a whole BUNCH of information, and I only touched a small part of it. For the interested, and also to stave off a wave of people saying "You didn't do any research!!1!11111" a categorized list of websites I used as sources can be found &lt;a title="Sources!" href="http://condejo.org/images/Verve%20Article/sources.txt" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;strong&gt;&lt;span style="font-size: x-large;"&gt;More Reading On the Subject&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;How to start your own MLM-Pyramid Scheme (sounds and smells like Verve!):&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mlm-thetruth.com/tools1/humor-satire/start-pyramid-scheme"&gt;http://mlm-thetruth.com/tools1/humor-satire/start-pyramid-scheme&lt;/a&gt;&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="font-size: x-large;"&gt;TL;DR&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For those that didn't want to read it all, I thank you for scrolling this far to find the TL;DR.&lt;/p&gt;
&lt;p&gt;TL;DR: Vemma is a MLM (Multi-Level-Marketing) company. They insinuate that you're able to make a boatload of money by working part-time. That's basically false, to make any money in the MLM business you have to be there at the inception of the venture and work your ass off. MLM companies request that you use your social networks to sell goods, preying off of the trust you've created with those people to make a buck. They're marketing a product that's very similar to competitors by saying it's akin to Ambrosia and can solve Divorce and Bankruptcy. You can make money with this snakewater if you work your butt off, but the statistical likelihood is that you won't.&lt;/p&gt;
&lt;p&gt;2/10 would not recommend.&lt;/p&gt;</content><category term="OpEds"></category><category term="Verve"></category><category term="MLM"></category><category term="Vemma"></category><category term="Scam"></category></entry><entry><title>An Internet of Things (Part 2): Calling Meteor.js Functions Externally</title><link href="https://connerswann.me/2014/06/an-internet-of-things-part-2-calling-meteorjs-functions-externally.html" rel="alternate"></link><published>2014-06-24T20:54:00-07:00</published><updated>2014-06-24T20:54:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2014-06-24:/2014/06/an-internet-of-things-part-2-calling-meteorjs-functions-externally.html</id><summary type="html">&lt;p&gt;In a previous post I talked about my simple Arduino sketch that contacts a Meteor.js server and feeds it a value read from a Photoresistor. Now I'm going to detail how I hacked together the server. Just an advanced warning, I'm not an expert with Meteor and I'm going …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a previous post I talked about my simple Arduino sketch that contacts a Meteor.js server and feeds it a value read from a Photoresistor. Now I'm going to detail how I hacked together the server. Just an advanced warning, I'm not an expert with Meteor and I'm going to be refining this implementation as I go. With that said, it works, so I've got that going for me.&lt;/p&gt;
&lt;p&gt;First, it's probably pertinent to go into a little detail as to what "Meteor.js" is. According to them:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Meteor is an open-source platform for building top-quality web apps in a fraction of the time, whether you're an expert developer or just getting started.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a nutshell, it's a javascript framework for writing web apps. One attribute that makes sets it apart is the ability to write both the client-side and server-side applications entirely in Javascript using the handy Meteor library. I think it adds an element of fun/coding/computer-science-ness to (what I consider to be) the drudgery of classic HTML and CSS.&lt;/p&gt;
&lt;p&gt;I created a simple Meteor application to interface with my Arduino. The page is tracking a database table called "reads" and whenever another item is added it updates itself with the value from that item's "value" entry in the database. All of this logic happens on the client-side and is contained the following code.&lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-javascript"&gt;
Reads = new Meteor.Collection("reads");

if (Meteor.isClient) {  
    // Fills in Templates
    Template.temperature.val = function () {
        val = Reads.find({}, {sort:{timestamp: -1}}, {limit: 1}).fetch()[0];
        if(val){
            Session.set("currentValue", val.value);
        }
        return val;
    };
    Template.temperature.message = function(){
        if(Session.get("currentValue") &lt; 200){
            return "Hey, who turned out the lights?";
        }
      else{
            return "It's totally bright enough to see..."
        }
    };
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;So far, this is all simple out-of-the-box meteor behavior, but since I wanted to play around with calling Meteor functions externally, I added Iron Router to my Meteor app. Iron Router is a "client and server-side router built for use with Meteor." It allows you to route traffic to URLs on your server (or client) to arbitrary templates or even trigger meteor functions.&lt;/p&gt;
&lt;p&gt;The code is super uninvolved, simply create a new Iron Router route on the server have it store the URL parameters that have been passed in a database. Meteor handles the ordeal of shuttling that data down to the client, something I find to be very powerful.&lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-javascript"&gt;
if (Meteor.isServer) {  
    Router.map(function () {
        this.route('yolo', {
            where: 'server',
            action: function () {
            time = new Date().getTime();
            Reads.insert({value: this.request.query.v, timestamp: time})
        }
    });
    });
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;I created a route at &lt;code&gt;http://arduinotest.meteor.com/yolo&lt;/code&gt;. You can see that it works for yourself, just copy this: &lt;code&gt;http://arduinotest.meteor.com/yolo?v=9001&lt;/code&gt; into your url bar and then visit &lt;code&gt;http://arduinotest.meteor.com&lt;/code&gt;. Assuming the previous value wasn't already "9001", the page should have updated to reflect the changes.&lt;/p&gt;
&lt;p&gt;The page actually reacts to the values that are passed, but you'll have to play with it to figure out exactly what makes the text change!&lt;/p&gt;</content><category term="Arduino"></category><category term="Arduino"></category><category term="IOT"></category><category term="REST"></category><category term="API"></category><category term="MeteorJS"></category><category term="Javascript"></category></entry><entry><title>Arduino - An Internet of Things</title><link href="https://connerswann.me/2014/06/arduino-an-internet-of-things.html" rel="alternate"></link><published>2014-06-24T20:51:00-07:00</published><updated>2014-06-24T20:51:00-07:00</updated><author><name>Conner Swann</name></author><id>tag:connerswann.me,2014-06-24:/2014/06/arduino-an-internet-of-things.html</id><summary type="html">&lt;p&gt;Recently I got an Arduino Uno, a small, versatile microcontroller that has been adopted by the "Maker" community as the controller of choice for hobby electronics, and for good reason.&lt;/p&gt;
&lt;p&gt;In my limited experience so far, Arduino is a real joy to deal with, having a very simplistic interface while …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Recently I got an Arduino Uno, a small, versatile microcontroller that has been adopted by the "Maker" community as the controller of choice for hobby electronics, and for good reason.&lt;/p&gt;
&lt;p&gt;In my limited experience so far, Arduino is a real joy to deal with, having a very simplistic interface while preserving a very robbust "skillset" if you will. The first thing I did was spend a day playing with a box of components, learning how to read sensors and analog components with the board and reacting to those readings, whether it be just reporting back to the computer via serial or prehaps turning on/off an LED.&lt;/p&gt;
&lt;p&gt;I've heard the term "Internet of Things" thrown around a lot, but never really quite understood what all the hype was about. However, after screwing around with an internet-equipped Arduino for a day, I can finally appreciate the power of being able to take your physical sensors and hardware and grant them a voice on the great forum that is "The Internet".&lt;/p&gt;
&lt;p&gt;To demonstrate this idea, I wired up a photoresistor to my Arduino to take readings of the ambient light, and had it report the values to a Meteor.js server every ten seconds. The code is below, if you get lost or confused, leave a comment, but it's pretty heavily commented and (hopefully) easy to follow.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo Of Project" src="https://connerswann.me/2014/06/arduino-1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: This is super-hacked together, I'm not claiming to be a guru. With that said, it works, so I have that going for me.&lt;/p&gt;
&lt;pre class="line-numbers"&gt;
&lt;code class="language-c"&gt;
#include &lt; SPI.h &gt;
#include &lt; Ethernet.h &gt;

// For ethernet shield
byte mac[] = { 0xDE, 0xAD, 0xBE, 0xEF, 0xFE, 0xED };// MAC Address  
char server[] = "arduinotest.meteor.com";            // Server to connect to  
// IP Address in case we don't get one from DCHP
IPAddress ip(192,168,0,177);    

EthernetClient client;

// Last time we connected to the server, in milliseconds
unsigned long lastConnectionTime = 0;  
// State of the connection last time through the main loop                
boolean lastConnected = false;  
// Delay between updates, in milliseconds
const unsigned long postingInterval = 5000;        

void setup() {  
    // Open the serial connection
    Serial.begin(9600);
    Serial.println("Initializing Ethernet Shield...");

    if (Ethernet.begin(mac) == 0){
        // DCHP config has failed
        Serial.println("Failed to configure Ethernet using DHCP");
        // Set it up manually
        Ethernet.begin(mac, ip);  
    }
    // Give it a second
    delay(1000);
    Serial.print("IP: ");
    Serial.println(Ethernet.localIP());
}

void loop() {  
    // In case we ever get a response, log it to serial
    // commented out because unnecessary
    // if (client.available()) {
    //  char c = client.read();
    //  Serial.println("RESPONSE INCOMING CAPTAIN!");
    //  Serial.println("==========================");
    //  Serial.print(c);
    //  Serial.println("==========================");
    // }

    // If it's not connected but we've connected recently
    if (!client.connected() &amp;&amp; lastConnected) {
        //Serial.println();
        Serial.println("trouble in paradise... disconnecting.");
        client.stop();
    }
    // if you're not connected, and ten seconds have passed since
    // your last connection, then connect again and send data:
    if(!client.connected() &amp;&amp; (millis() - lastConnectionTime &gt; postingInterval)) {
        httpRequest();
    }
    // store the state of the connection for next time through
    // the loop:
    lastConnected = client.connected();
}

int httpRequest() {  
    // if there's a successful connection:
    // String postData = String("v=" + analogRead(0));
    if (client.connect(server, 80)) {
        Serial.println("connecting...");
        // send the HTTP POST request:
        client.println(constructRequest());
        client.println("Host: arduinotest.meteor.com");
        client.println("User-Agent: jetpack-rinocerous");
        client.println("Connection: close");
        client.println();


        // let us know it sent
        Serial.println("Sent!");
        // note the time that the connection was made:
        lastConnectionTime = millis();
    } 
    else {
        // if you couldn't make a connection:
        Serial.println("connection failed");
        Serial.println("disconnecting.");
        client.stop();
    }
    return 1;
}

String constructRequest(){  
    String post = "POST ";
    String location = "/yolo?v=";
    int val = analogRead(0);
    String type = " HTTP/1.1";

    return post + location + val + type;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In english, the Arduino initializes the Ethernet shield and requests an IP address via DCHP. Assuming that's successful, it enters the loop and periodically tries to connect to my server at arduinotest.meteor.com.&lt;/p&gt;
&lt;p&gt;Once connected, about every ten seconds (depending on if the last connection attempt was sucessful or not) it reads the photoresistor and sends a request to the server with the value as a url parameter. It then loops again. The algorithm's also got contingencies for what happens if it can't connect, if you're interested take a look at the code.&lt;/p&gt;
&lt;p&gt;That's it for the Arduino code. Once I've added to the Meteor.js server-side code a little bit, I'll make a post about how I accomplished that.&lt;/p&gt;</content><category term="Arduino"></category><category term="Arduino"></category><category term="IOT"></category><category term="REST"></category><category term="API"></category><category term="Javascript"></category></entry></feed>